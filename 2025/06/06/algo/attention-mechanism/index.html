<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>attention mechanism | Hexo</title><meta name="author" content="Bo Han"><meta name="copyright" content="Bo Han"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Transformer 的向量化与 Attention 机制解析 参考视频 向量化（Embedding） 向量化是将输入文本中的单词或子词（Token）转化为高维向量（Embedding）的过程。每个 Token 会被映射到一个高维空间中的向量，用于表示其语义信息。 在 Transformer 的原始论文中，Embedding 的维度通常设为 512： $$ dim_emb &#x3D; 512 $$  初">
<meta property="og:type" content="article">
<meta property="og:title" content="attention mechanism">
<meta property="og:url" content="https://lingwu-hb.github.io/2025/06/06/algo/attention-mechanism/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Transformer 的向量化与 Attention 机制解析 参考视频 向量化（Embedding） 向量化是将输入文本中的单词或子词（Token）转化为高维向量（Embedding）的过程。每个 Token 会被映射到一个高维空间中的向量，用于表示其语义信息。 在 Transformer 的原始论文中，Embedding 的维度通常设为 512： $$ dim_emb &#x3D; 512 $$  初">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://lingwu-hb.github.io/img/yaojie.jpg">
<meta property="article:published_time" content="2025-06-06T02:05:14.000Z">
<meta property="article:modified_time" content="2025-11-04T11:50:50.888Z">
<meta property="article:author" content="Bo Han">
<meta property="article:tag" content="Attention">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lingwu-hb.github.io/img/yaojie.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "attention mechanism",
  "url": "https://lingwu-hb.github.io/2025/06/06/algo/attention-mechanism/",
  "image": "https://lingwu-hb.github.io/img/yaojie.jpg",
  "datePublished": "2025-06-06T02:05:14.000Z",
  "dateModified": "2025-11-04T11:50:50.888Z",
  "author": [
    {
      "@type": "Person",
      "name": "Bo Han",
      "url": "https://lingwu-hb.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://lingwu-hb.github.io/2025/06/06/algo/attention-mechanism/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;700&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'attention mechanism',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Hexo</span></a><a class="nav-page-title" href="/"><span class="site-name">attention mechanism</span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">attention mechanism</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-06-06T02:05:14.000Z" title="Created 2025-06-06 10:05:14">2025-06-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-11-04T11:50:50.888Z" title="Updated 2025-11-04 19:50:50">2025-11-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Transformer-的向量化与-Attention-机制解析">Transformer 的向量化与 Attention 机制解析</h1>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1TZ421j7Ke/?spm_id_from=333.337.search-card.all.click&amp;vd_source=dc1f57f4d0f3636b7a85bb3e052b778f">参考视频</a></p>
<h2 id="向量化（Embedding）">向量化（Embedding）</h2>
<p>向量化是将输入文本中的单词或子词（Token）转化为高维向量（Embedding）的过程。每个 Token 会被映射到一个高维空间中的向量，用于表示其语义信息。</p>
<p>在 Transformer 的原始论文中，Embedding 的维度通常设为 512：<br>
$$<br>
dim_emb = 512<br>
$$</p>
<ul>
<li><strong>初始 Embedding</strong>：相同的 Token（例如同一个单词）会被赋予相同的初始 Embedding 向量。</li>
<li><strong>上下文调整</strong>：由于同一个单词在不同句子或同一句子的不同位置可能具有不同含义，Transformer 通过后续的 Attention 机制和位置编码（Positional Encoding）调整每个 Token 的 Embedding，使其不仅表示单词本身，还能捕捉丰富的上下文信息。</li>
</ul>
<p><strong>直观理解</strong>：你可以将 Embedding 想象成一个高维空间中的“坐标点”，每个 Token 是一个点，而 Transformer 的任务是通过上下文调整这些点的“位置”，让它们更准确地反映语义。</p>
<hr>
<h2 id="Attention-机制">Attention 机制</h2>
<p>Attention 机制是 Transformer 的核心，用于计算 Token 之间的相关性，并根据上下文调整 Embedding。核心公式如下：<br>
$$<br>
Attention(Q, K, V ) = softmax( QK^T / √d_k )V<br>
$$</p>
<h3 id="Q、K、V-向量的生成">Q、K、V 向量的生成</h3>
<p>下文讨论的情况都默认 <code>batch_num = 1</code> 的情况，在实际情况下，Q，K 和 V 矩阵都是多批次的三位向量。在 <code>pytorch</code> 进行矩阵乘法运算的时候，会自动将多批次进行并行计算，所以只需要考虑后两个维度即可。</p>
<ul>
<li>
<p>Q（Query）与 K（Key）向量：<br>
$$ Q = W_kE $$<br>
$$ K = W_kE $$</p>
<ul>
<li>其中，$E$ 是输入的 Embedding 向量，维度为 $d_{emb}$（如 512）。</li>
<li>$W_q$ 和 $W_k$ 是可学习的权重矩阵，维度为 $d_k \times d_{emb}$，通常 $d_k &lt; d_{emb}$，从而将 Embedding 映射到较低维度的空间。</li>
<li>$Q$ 和 $K$ 的点积（$QK^T$）计算了每个 Token 对其他 Token 的相关性。点积值越大，表示两个 Token 的语义关联越强。</li>
</ul>
</li>
<li>
<p>V（Value）向量：<br>
$$<br>
V = W_vE<br>
$$</p>
<ul>
<li>$W_v$ 是另一个可学习的权重矩阵，维度同样为 $d_k \times d_{emb}$。</li>
<li>$V$ 表示 Token 的语义内容，经过 Attention 机制的加权后，生成新的上下文感知的向量表示。</li>
</ul>
</li>
</ul>
<h3 id="归一化与缩放">归一化与缩放</h3>
<ul>
<li>归一化</li>
</ul>
<p>归一化使用的是 layerNorm，与其他归一化方法（如 Batch Normalization）不同，Layer Normalization 对每个样本的<strong>特征维度</strong>进行归一化，而不是跨批次归一化。</p>
<p>在 Transformer 中，Layer Normalization 通常作用于每个 token 的嵌入向量。例如，对于输入张量(n, seq_n, d)（批次大小为 n，序列长度为 seq_n，嵌入维度为 d），Layer Normalization 对每个样本的每个 token 的 ddd 维向量独立进行归一化。</p>
<p>具体来说，归一化操作沿最后一个维度（即 d）计算均值和方差，因此对批次大小 n 和序列长度 seq_n 无依赖。</p>
<ul>
<li>$\sqrt(k)$ 缩放</li>
</ul>
<p>$QK^T$ 计算得到相关性矩阵，经过 $\sqrt{d_k}$ 缩放（防止点积值过大导致 softmax 梯度消失）。</p>
<blockquote>
<p>❗❗ ❗一般来说，当 seq_n 和 dk 大小比较大的时候，点积完成之后的数值比较大，softmax 容易导致梯度消失，所以需要进行缩放。如果比较小的情况，可以不进行缩放</p>
</blockquote>
<ul>
<li>$\text{softmax}$ 将相关性矩阵归一化为 0 到 1 之间的权重，称为“注意力分数”。</li>
<li>最后，注意力分数与 $V$ 相乘，得到加权后的输出，融合了上下文信息。</li>
</ul>
<hr>
<h2 id="通俗理解">通俗理解</h2>
<p>Attention 机制的本质是通过计算 Token 之间的相关性，动态调整每个 Token 的表示，使其包含上下文信息。我们可以简化公式来理解其核心思想：</p>
<p>参考文章：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/298810062">Attention 机制解析</a></p>
<p>忽略权重矩阵 $W_q$、$W_k$、$W_v$ 和缩放因子，公式简化为：<br>
$$<br>
Attention(Q, K, V ) = softmax( QK^T / √d_k )V \approx QK^TV \approx W_qE(WkE)^TW_vE \approx EE^TE<br>
$$</p>
<ul>
<li><strong>$E E^T$（注意力矩阵）</strong>：表示每个 Token 与其他 Token 的相似度。相似度越高，说明两个 Token 越相关，分配的“注意力”越多。</li>
<li><strong>$\text{softmax}(E E^T)$</strong>：将相似度归一化为注意力分数，类似于概率分布。</li>
<li><strong>乘以 $E$</strong>：用注意力分数对原始 Embedding 进行加权平均，生成新的向量表示，融合了上下文信息。</li>
</ul>
<p><strong>类比</strong>：想象一群人在开会，每个人（Token）都在“听”其他人（通过 $E E^T$ 计算相关性），然后根据“听到的内容”（注意力分数）调整自己的发言（Embedding），最终形成更符合整体讨论的观点。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://lingwu-hb.github.io">Bo Han</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://lingwu-hb.github.io/2025/06/06/algo/attention-mechanism/">https://lingwu-hb.github.io/2025/06/06/algo/attention-mechanism/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Attention/">Attention</a></div><div class="post-share"><div class="social-share" data-image="/img/yaojie.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/05/28/paper/thesis-writing-guidance/" title="thesis writing guidance"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">thesis writing guidance</div></div><div class="info-2"><div class="info-item-1">学术规范与论文写作课程 考核形式：结业作业将基于真实英文论文，完成一次“模拟审稿与重写训练” 具体内容：阅读并评估一篇论文；识别其中的问题与亮点；提出批判性意见或修改建议；用你自己的语言重新表达 注意：不考察语言技巧，而看思维能力与判断力；每个人选择的论文不同，允许个性化表达 论文审稿 各部分审稿要点  引言部分：重点审查作者是否清晰介绍了研究背景，引用了足够多的以往工作，并对问题目前的发展水平有概括性介绍；是否明确阐述了本文的研究目的和大致框架。 方法论部分：对于新手来说，提出技术性问题较难，可着重检查作者的推导过程是否有明显错误、公式的格式是否规范等。 实验/仿真部分：关注作者是否清晰写明了实验条件设置，实验对照是否合理，实验结果的图表是否清晰、规范，以及对实验结果的分析是否充分、合理。 总结部分：审查是否对整篇文章进行了反思总结，是否说明了文章做了什么、效果如何、验证了什么，以及未来可以继续补充/深入哪方面的工作。  表达模板 一、对论文整体的概括性表达  “The paper is very well written, and contributes...</div></div></div></a><a class="pagination-related" href="/2025/06/11/%E9%9A%8F%E7%AC%94/useful-tools/" title="useful tools"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">useful tools</div></div><div class="info-2"><div class="info-item-1">常用工具 配置 zsh &amp;&amp; oh-my-zsh  ZSH（Z-Shell）是一种功能强大的 Unix shell，提供了许多增强功能，如高级脚本能力、自动补全和主题支持。  sudo apt install zsh  Oh-My-Zsh 是一个用于管理 ZSH 配置的流行框架，提供了丰富的主题和插件。使用以下命令安装  sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot;  插件安装  git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-syntax-highlighting git clone https://github.com/zsh-users/zsh-autosuggestions...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/yaojie.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Bo Han</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">33</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">35</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer-%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%E4%B8%8E-Attention-%E6%9C%BA%E5%88%B6%E8%A7%A3%E6%9E%90"><span class="toc-number">1.</span> <span class="toc-text">Transformer 的向量化与 Attention 机制解析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%88Embedding%EF%BC%89"><span class="toc-number">1.1.</span> <span class="toc-text">向量化（Embedding）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Attention-%E6%9C%BA%E5%88%B6"><span class="toc-number">1.2.</span> <span class="toc-text">Attention 机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Q%E3%80%81K%E3%80%81V-%E5%90%91%E9%87%8F%E7%9A%84%E7%94%9F%E6%88%90"><span class="toc-number">1.2.1.</span> <span class="toc-text">Q、K、V 向量的生成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E4%B8%8E%E7%BC%A9%E6%94%BE"><span class="toc-number">1.2.2.</span> <span class="toc-text">归一化与缩放</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%9A%E4%BF%97%E7%90%86%E8%A7%A3"><span class="toc-number">1.3.</span> <span class="toc-text">通俗理解</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/09/AICache/thirdStage/Mithril-intro/" title="Mithril intro">Mithril intro</a><time datetime="2025-07-09T08:40:28.000Z" title="Created 2025-07-09 16:40:28">2025-07-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/04/AICache/thirdStage/RL-paper-survey/" title="RL paper survey">RL paper survey</a><time datetime="2025-07-04T02:03:02.000Z" title="Created 2025-07-04 10:03:02">2025-07-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/03/AICache/thirdStage/tsPrefetchus/" title="tsPrefetchus">tsPrefetchus</a><time datetime="2025-07-03T06:21:16.000Z" title="Created 2025-07-03 14:21:16">2025-07-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/03/AICache/thirdStage/cache-and-request-contrast/" title="cache and request contrast">cache and request contrast</a><time datetime="2025-07-03T01:20:11.000Z" title="Created 2025-07-03 09:20:11">2025-07-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/01/AICache/thirdStage/spdk/" title="spdk">spdk</a><time datetime="2025-07-01T01:55:48.000Z" title="Created 2025-07-01 09:55:48">2025-07-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Bo Han</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.2.0</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>
---
title: 前期调研
date: 2025-06-29 13:00:30
categories:
  - AI
  - data storage
tags:
  - 未分类
---




## 确定大的研究方向

主要背景图如下所示：

![](/img/开题思路.jpeg)

## 具体时间节点

三个月时间

确定题目：一个半月

文献调研 + 材料整理：一个半月

### 目前工作重点

1. 确定访问模式，最好能根据 cacheus 的方法进行实验验证

开展实验的话，是否需要租用 GPU？了解一下 cacheus 分析访问模式的方法，用于参考学习

2. 相关文献调研，判断业界目前采用何种方式进行预取？

主要需要和传统块存储进行对比，大概率大模型 prefill 和 inference 阶段有其独特的访问模式！

  
主要方向：KV Cache 缓存管理加速推理速度方向，不限定为一定要预取！

后面阅读文章的时候需要重点关注：文章的问题总结出来，文章主要解决哪些问题，还有哪些没有解决的问题，从问题中发现创新点。

‼️‼️‼️ 当前需要工作内容

1. 阅读 《3.3　智能显存分配与预取》中提及的相关论文，然后提取相关内容到重点论文分析中。

## 开题思路

### 相关问题

1. 实验室大方向是什么？能不能搞 LLM 推理加速
2. 实验室有哪些人在搞大模型方向，具体是哪个方向
3. LLM 背景知识不足，开题的题目不太能想出 idea

4. LLM KV Cache 与传统的存储预取差异比较大，涉及大模型内部原理较多。

训练一个轻量级的机器学习模型（如小型神经网络或决策树），以第 $i-1$ 层的注意力输入和模型参数为输入，预测第 $i$ 层中哪些 token 的 KV cache 条目最重要。

## 相关概念整理

KVCache 是什么？

在 Transformer 的推理过程中，生成一个新的 Token 需要依赖之前过程中生成的 K 和 V 矩阵。为了减少重复计算，需要将每次生成的 K 和 V 矩阵保存起来，用于保存这两个矩阵的存储空间就是 KVCache

目前有什么问题，需要进行什么优化？

核心问题：KVCache 容量不够，专业级的单 GPU 最大显存也就是只有100G左右。但是模型参数量 + KV 矩阵在模型比较大的时候，显存空间容量会显著不足。

### 训练和推理中 KVCache 不同的作用

KVCache 的作用局限于前向传播中保存 K V 矩阵，用于减少重复计算。

**训练**过程 KV Cache 的作用相对有限，因为训练涉及反向传播，缓存的键值对通常仅在当前 batch 的前向传播中使用，且需要额外存储梯度和优化器状态，整体内存压力较大。训练时，KV Cache 通常按 batch 管理，每个 batch 的缓存独立存储，且在每次迭代后可能被清空或更新。

但是在**推理**过程中，KV Cache 在自回归生成（如文本生成）中尤为重要，因为推理通常是逐 token 生成，输入序列动态增长。

**主要区别：**

- **训练**

- **参数优化的过程，计算密集、内存需求高，涉及批量数据和多次迭代**
- **KV Cache 减少重复计算，提升批量处理效率，但作用受限于反向传播和内存压力。**

- **推理**

- **预测过程，注重低延迟和高效生成，通常处理单条输入**
- **KV Cache 是加速自回归生成的核心，显著降低延迟，但需优化内存以支持长序列生成。**

### KVCache 访问模式

kvcache 访问规律是什么样子的？访问模式如何？尝试使用 cacheus 里面的分析方法分析一下 kvcache 的访问规律。

‼️‼️‼️ 以下内容由 AI 生成，需要进行甄别

1. LLM对KVCache的访问模式  
    以下是LLM在训练和推理中对KVCache（主要涉及键值缓存和激活量）的访问模式规律：

训练阶段的KVCache访问模式

- 键值缓存：

- 在标准训练中，键值缓存的使用较少，因为训练通常是全序列的前向和反向传播，键值对在每次迭代中重新计算，不需要持久化存储。
- 某些优化技术（如梯度检查点或激活量重计算）可能涉及部分键值对的临时缓存。这种情况下，KVCache的访问模式表现为：

- 写入模式：在每个前向传播阶段，键值对按层生成并写入缓存，属于顺序写入，数据量与序列长度和模型层数成正比。
- 读取模式：在反向传播时，可能需要读取部分缓存的键值对或激活量，读取模式可能是随机或部分顺序访问，取决于优化策略（如检查点选择）。

- 激活量：

- 激活量在训练中占用大量内存，尤其在深层Transformer模型中。访问模式包括：

- 高频写入：前向传播时，各层激活量按顺序生成并存储。
- 高频读取：反向传播时，需要读取激活量计算梯度，访问模式通常是逆序读取。
- 内存压力：由于激活量数据量大，训练中常使用梯度检查点技术，仅缓存部分激活量，减少内存占用，但增加计算开销。

- 模型权重：

- 权重通常存储在GPU内存中，访问模式为高频读取（前向/反向传播）和低频更新（梯度更新）。在分布式训练中，权重的分片可能导致跨节点的随机访问。

推理阶段的KVCache访问模式

- 键值缓存：

- 在推理阶段（特别是自回归生成），KVCache是性能优化的核心，用于缓存已计算的键值对，避免重复计算。访问模式包括：

- 顺序写入：在生成每个新token时，模型计算当前token的键值对并追加到KVCache中。写入模式是顺序的，按时间步（timestep）递增。
- 顺序读取：生成新token时，注意力机制需要读取整个KVCache中的历史键值对，计算当前token的注意力输出。读取模式通常是顺序的，但随着序列长度增加，读取的数据量线性增长。
- 内存增长：KVCache的大小随序列长度线性增加（O(sequence_length * num_layers * hidden_size)），对内存带宽和容量要求高。
- 优化场景：在长序列生成中，可能使用滑动窗口或稀疏注意力机制来限制KVCache的内存占用，这会改变访问模式为部分随机访问（仅读取窗口内的键值对）。

- 激活量：

- 推理中激活量的存储需求较低，因为无需反向传播。访问模式主要为：

- 临时写入：每层前向传播生成激活量，写入模式为顺序写入。

- 有限读取：激活量通常在当前时间步内使用，读取频率较低，模式为局部顺序访问。

- 模型权重：

- 权重在推理中是只读的，访问模式为高频顺序读取，按层加载到计算单元。权重大小固定，但若使用权重卸载（offloading）技术，可能涉及从存储到内存的随机访问。

2. KVCache访问模式的规律总结

- 训练阶段：

- 键值缓存：使用较少，主要是临时缓存，表现为顺序写入和部分随机读取，频率较低。
- 激活量：高频顺序写入和逆序读取，内存占用大，常通过检查点技术优化。
- 模型权重：高频顺序读取，低频更新，分布式训练中可能有跨节点随机访问。

- 推理阶段：

- 键值缓存：核心组件，表现为顺序写入和顺序读取，数据量随序列长度线性增长。长序列生成可能引入滑动窗口的随机访问。
- 激活量：临时顺序写入和局部读取，内存需求较低。
- 模型权重：高频顺序读取，若有权重卸载则可能有随机访问。

### 推理过程中的预填充和解码阶段

预填充阶段生成首个输出标记及其键值缓存， 可以并行处理输入序列中的所有标记，首个生成标 记的延迟受到GPU每秒浮点运算次数（floating-point operations per second，FLOPS）的限制，属于**计算密集 型任务**. 而解码阶段以自回归的方式单次单个地逐 步生成输出标记，当前的输出标记依赖于上一个标 记和旧的键值缓存. 这种顺序生成过程使得工作负 载 受 显 存 带 宽 限 制， 属 于 **带 宽 密 集 型 任 务**.  由 于 Transformer的推理过程是自回归的，这意味着在推 理过程中必须跟踪和存储先前的状态，进而增加了显存需求.

### 大模型中预取 vs 传统存储系统预取

传统存储系统预取一般来说**主要依赖于外部观察到的数据访问行为，通常不涉及对应用程序内部计算逻辑的深入理解。**预取的目标是减少存储访问延迟（如从磁盘到内存，或从内存到缓存），但其准确性受限于访问模式的规律性和可预测性。

大模型中预取**更多是基于模型内在机制的预取**。这种预取方式使得其不需要积累历史访问数据，而是直接依赖实时的模型计算特性，也就是说，大模型中预取会关注数据的语义，相比之下，传统预取更关注数据访问的物理或逻辑局部性，而不理解数据的语义内容。

## 重点论文分析

这部分会拿出一些相关性比较强的论文进行单独分析

这部分文章都是基于异质存储介质加速 LLM 的训练和推理过程

参考：《大语言模型推理中的存储优化技术综述》 3.3 节部分

### FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU

提供了一个应用多级存储的范式

FlexGen 是下面这篇文章[InfiniGen]的基础，这篇文章重点关注 LLM 推理，目标是最大程度得提升 LLM 推理过程中的吞吐量。主要采用了以下几种方法：

1. 采用 zig-zag 计算模式。与传统的逐层（layer-by-layer）计算模式不同，FlexGen 采用“之字形块调度”（zig-zag block schedule），**在同一 Transformer 层内优先处理多个批次（batches），而不是逐层遍历整个模型。**这种模式利用了同一层计算可以共享权重矩阵的特点，减少频繁加载权重的 I/O 开销。
2. FlexGen 利用多级存储架构（memory hierarchy），通过卸载（offloading）权重、KV 缓存和激活值到 CPU 或磁盘，释放 GPU 显存，支持大 batch size。同时，通过 overlapping（重叠） 技术，让 GPU 计算与 I/O 操作（数据加载/存储）并行，隐藏 CPU 内存和磁盘的访问延迟，最大化 GPU 利用率。
3. FlexGen 使用 4 位量化（Int4 quantization） 和 稀疏注意力（sparse attention） 压缩权重和 KV 缓存，减少内存占用和 I/O 数据量，从而支持更大批次（batch size），提升吞吐量。这些策略在不需重新训练或校准的情况下，保持模型精度。

### InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management

核心思想：利用 key 和 query 矩阵中的 outlier 特性（即某些维度比其他维度的值大很多），这种特性是由于模型的内在属性引起的。通过仅选择部分重要维度（partial key 和 partial query）来推测注意力模式（attention pattern），从而减少 KV cache 的传输量和计算量。

#### 针对 FlexGen 的优化工作

FlexGen 的方法：

- FlexGen 将 KV cache 视为统一的张量，根据线性规划分配到 GPU、CPU 或磁盘（Page 5, Section 4.2）。其管理策略是静态的，未考虑 token 在不同迭代或层的动态重要性。
- 稀疏注意力通过简单的 Top-K 选择（top-10% 的注意力值，Page 7），但未动态调整所选 token，且 KV cache 的加载仍需大量 I/O。

InfiniGen 的改进：

- **动态 token 选择**：InfiniGen 提出了一种基于注意力模式推测（speculation）的动态 KV cache 管理机制。通过分析 query 和 key 矩阵中的 outlier 维度，利用第 i-1 层的注意力输入推测第 i 层的注意力分数，选择高分数的 token（约 10% 的 KV cache）进行预取。

以下步骤与第 i 层的推理计算并行进行，提前预取出 i+1 层计算所需要的部分 K，V 缓存。

1. 通过离线 SVD，确定 Q 和 K 矩阵中 30% 左右的 outlier，生成正交矩阵 A (512 × 1536）
2. 在计算第 i 层注意力机制的同时，利用 X^i 和 partial Q 和 paritial K 矩阵，推测出第 i+1 层的注意力分数矩阵。
3. 选择注意力分数矩阵中大于阈值的部分数值所对应的 Token，预取这些 Token 对应的 K^i+1 和 V^i+1，从而减少 IO 加载。

### RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation

### CachedAttention: Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention

## 代码实践

### FlexLLMGen

参考 flex_opt.py

一般来说，模型推理的代码主要具有以下流程：

1. 超参数设置
2. 模型初始化

初始化了一些缓存空间，用于为异步传输做准备

3. 模型推理（重点）

```
def generation_loop_overlap_multi_batch(self):
    # Prologue
    for k in range(self.num_gpu_batches):
        self.load_weight(0, 0, k)
        self.load_hidden(0, 0, 0)
    self.sync()

    # Generate
    for i in range(self.execute_gen_len):
        timers("generate").start()
        for k in range(self.num_gpu_batches):
            self.update_attention_mask(i, k)
            for j in range(self.num_layers):
                for k in range(self.num_gpu_batches):
                    # 下面这些操作会异步执行
                    self.load_weight(i, j+1, k) # 从磁盘加载权重
                    self.load_cache(i, j, k+1) # 从磁盘加载cache
                    self.store_hidden(i, j, k-1) # 从磁盘加载hidden
                    self.load_hidden(i, j, k+1) # 从磁盘加载hidden
                    self.compute_layer(i, j, k) # 计算layer
                    self.store_cache(i, j, k-1) # 从磁盘加载cache
                    self.sync() # 同步
        timers("generate").stop()

        # Epilogue
        self.store_hidden(
            self.execute_gen_len-1, self.num_layers-1, self.num_gpu_batches-1)
```

Zig-Zag计算模式：通过generation_loop_overlap_multi_batch方法实现，优先在同一Transformer层内处理多个批次，以减少权重加载的I/O开销。

多级存储架构与卸载：通过Policy类的百分比参数和TorchDevice、TorchDisk、TorchMixedDevice类实现，使得模型的权重、KV缓存和激活值可以灵活地分布在GPU、CPU和磁盘之间。同时，通过overlap参数和相关方法实现I/O与计算的重叠执行。

压缩技术：通过CompressionConfig和TorchCompressedDevice实现4位量化，通过attn_sparsity参数和_sparse_attention_value方法实现稀疏注意力，从而减少内存占用和I/O数据量。

‼️‼️FlexLLMGen 非常侧重系统级开发策略，尤其重视内存优化技术，后续研究的时候需要重点理解。

### InfiniGen

infiniGen 开源代码[GitHub - FMInference/FlexLLMGen: Running large language models on a single GPU for throughput-oriented scenarios.](https://github.com/FMInference/FlexLLMGen)

基于 FlexLLMGen 开源框架进行开发，FlexLLMGen 貌是一个非常成熟和稳定的框架，其中提供了多级存储的管理和并行策略。

## 参考资料

[1] [大模型推理加速：看图学KV Cache](https://zhuanlan.zhihu.com/p/662498827)

[2] [左鹏飞，CachedAttention](https://mp.weixin.qq.com/s?__biz=MzkyMDU5NDI3OQ==&mid=2247490516&idx=1&sn=94508eb225d48fa03563573bf0a7e7bb&chksm=c0e2cba206e0bfc06921dd218149780b6045a9646b59b7391ef9d5816718f00e01577584ed42&scene=126&sessionid=1748509848#rd)

1. **大语言模型推理中的存储优化技术综述**

从三个方面展开讨论：缓存架构管理、KV数据压缩、异构介质缓存（其中提到了智能预取策略）

`这篇综述涉及面相对比较广泛，**后续可以仔细阅读**`

KV 数据压缩中提到减少不重要的 KV 矩阵缓存，相当于传统缓存系统中直接砍掉 Trace 中不重要的请求，这明显不适用于我们的优化方法。因此可能只能考虑使用异构介质缓存情况下的优化（包括 CachedAttention 文章和智能预取策略）

2. 大模型时代下的存储系统挑战与技术发展

更加侧重于缓存扩容（也就是异构介质缓存）中需要解决的问题

以及涉及的关于缓存容错性的问题（关系不是特别大，可以不重点考虑）

3. AI Computing Systems for Large Language Models Training

整体比较侧重于讨论分布式环境下的问题和技术，其中 4.3.2 部分提到了异质存储中的卸载存储技术。

4. A Survey on Large Language Model Acceleration based on KV Cache Management

这篇调研对 KV 缓存管理的分类更加详细，细分了很多点，**后面可以仔细阅读**

主要包括以下几个点：

- KV Cache Selection
- System-level Optimization

5. 从BERT到ChatGPT：大模型训练中的存储系统挑战与技术发展

读完这篇之后，再仔细阅读 1. 中的关键内容

6. [Understanding the Workload Characteristics of Large Language Model Development](https://www.usenix.org/publications/loginonline/understanding-workload-characteristics-large-language-model-development)

这篇文章分析了 LLM 开发过程中对于资源的需求情况，并没有重点强调 KVCache 的访问模式。整体上和存储系统关联不是很大，需要很强的 LLM 背景知识才能看明白。

这篇文章主要分析了 LLM 和传统 DL 开发过程中的工作负载区别。LLM 背景下，工作负载指的是为了完成模型开发所需的全部任务和资源消耗。包括：计算任务、存储需求、通信需求、资源利用。

7. A Survey of LLM × DATA （刘老师群里发的 LLM × DATA 系统性综述）

只需要关注其中的 2.4.6 Data Storage for LLM - KVCache 即可。

内容和第 1 篇综述内容相似，只是这一篇文章涉及的范围更加广泛

8. [Flexgen LLM推理 CPU Offload计算架构到底干了什么事情？](Flexgen%20LLM推理%20CPU%20Offload计算架构到底干了什么事情？)
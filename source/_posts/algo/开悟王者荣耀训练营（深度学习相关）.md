---
title: 开悟王者荣耀训练营学习内容
date: 2024-08-04 15:25:47
categories:
- AI
tags:
- 强化学习
- DQN
- PPO
- 模型量化
---

### 开悟训练营学习内容

KaiwuDRL框架是腾讯开悟基于python研发的用于异步的off-policy强化学习训练的分布式框架，可以稳定高效地完成王者荣耀场景下的强化学习相关任务。

![](https://doc.aiarena.tencent.com/kaiwu-arena/gorgewalk_v2/latest/assets/images/kaiwuDRL_framework-5dfa599b4f303143984e9c1201729b6f.png)

图1：KaiwuDRL框架的核心功能模块



AI Server从Battle Server中直接获取到的信息为：

1. 当前帧数
2. 英雄信息
3. 地图中物件的信息

物件分为两种，宝箱和增益Buff。具体细节如下图所示：

![SingleReq结构UML图](D:\postGraRel\Competition\tencentKaiWuAlgorithm\img\SingleReq结构UML图.png)

图2：SingleReq数据结构UML图



AI Server为Battle Server和agent之间的桥梁，为两端进行数据传输。

其中，我们需要重点考察的为特征处理部分内容和奖励设置部分，相关修改策略见腾讯开悟文档。

* 特征处理

特征处理包括常规处理（app/gorge_walk/state_action_reward/gorge_walk_obs_reward.py）和神经网络的处理（app/gorge_walk/algorithm/torch_network.py）中的CNN网络。

* 奖励设置

奖励部分见（app/gorge_walk/state_action_reward/gorge_walk_obs_reward.py）



#### 代码执行流程分析

以Learner为例，分析app/gorge_walk中代码具体如何与框架中代码进行交互

1. 启动相关进程

```python
# app/gorge_walk/train_test.py
# 启动aisrv, learner, actor, battlesrv
procs : List[Process] = []
procs.append(Process(target=learner.main, name="learner"))
procs.append(Process(target=actor.main, name="actor"))
procs.append(Process(target=aisrv.main, name="aisrv"))
procs.append(Process(target=battlesrv.main, name='battlesrv'))
```

2. 加载相关神经网络和模型

```python
# framework/server/python/learner/on_policy_trainer.py
# network (for DQN, the network is BaseCNNNetwork in app/gorge_walk/algorithm/torch_network.py)
network = self.policy_conf.learner_network(
    self.policy_conf.state.state_space(),
    self.policy_conf.action.action_space()
)

# model
name = "%s_%s" % (CONFIG.app, CONFIG.algo)
# for DQN, the model is DQN model in app/gorge_walk/algorithm/torch_dqn_model.py
self.model = AlgoConf[CONFIG.algo].learner_model(
    network, name, CONFIG.svr_name)
```

3. Learner开始执行

```python
loop() -> run_once() -> self.train() -> self.model_wrapper.train() -> DQNmodel.learn()
```





#### Q&A

Q：目前需要做的工作就是：修改奖励设置 + 修改算法模型？具体而言，应该朝着什么方向去修改呢？



Q：如何设置奖励和惩罚的数值呢？完全按照经验嘛？



Q：这个项目的重点是奖励的设置？还是算法的修改？哪一部分对算法效果的影响更大？



Q：定义了一个BaseNetwork和一个BaseCNNNetwork，后者用了作为特征提取，那该网络的输出是什么呢？又是在什么地方使用了该模型的输出呢？前者都作为其他模型（DQN、PPO以及diy）的一部分

A：网络和模型不同，模型用于测试和训练，一般由多个网络组成。网络继承自nn.Module虚拟类，只要实现了__init__()和forward()即可。





### DQN算法学习理解

DQN（Deep Q-Network）算法基于Q-learning算法，使用了神经网络替代了Q-learning算法中的Q Tables。

- Q-learning算法简介

Q Tables表示为Q(s, a)，即在状态s下，执行动作a，**所期望获得的未来奖励**。

Q-learning算法的核心就在于**不断得更新Q Tables矩阵，直至Q Tables值得到收敛。**

在每个时间步，智能体都需要选择一个行动。这可以通过一种叫做ε-greedy策略的方法来完成，该方法会在探索（随机选择行动）和利用（选择当前Q-value最高的行动）之间进行权衡。

当智能体选择一个动作执行，并得到结果之后，智能体就按照贝尔曼等式对Q Tables进行更新。

- DQN算法

当环境中的状态和动作非常多的时候，Q Tables无法将所有的状态-动作对所表示的奖励值都存储起来。使用深度网络模型代替Q Tables，深度网络的输入为状态s，输出为对应各个可能动作的Q值。

通过这种方式，我们就可以在连续的状态空间和大规模的动作空间中工作。（**相当于把一个Q Tables，变成了一个Q函数，函数所对应的输入当然是连续的！**）

[DQN参考博客](https://www.cnblogs.com/xoslh/p/17609512.html#:~:text=DQN，全称Deep Q-Network，是一种强化学习算法，由DeepMind于2015年首次提出。,它结合了深度学习和Q学习两种技术，可以解决具有大量状态和动作的复杂问题。 在传统的Q-learning中，我们用一个表（Q-table）来存储每个状态-动作对的Q值。)



在开悟模型中，Actor与AI Server进行交互，拿到当前环境的状态，然后利用模型算法进行预测，产生next_state；Actor本身并不会进行训练更新模型参数，而是周期性得负责Learner模型的参数。

Learner从AI Server处拿到批量样本，训练模型并更新参数（一般使用梯度下降算法），然后周期性同步到Actor中。



对于DQN算法的实现，在框架代码中，需要**理解Trainer的作用**，同时，对于DQNTrainer的实现，框架代码使用了**某种设计模式**，对Trainer进行了层层嵌套。

从上往下，分为三层

```python
class GorgeWalkDQNTrainer(OnPolicyTrainer)
class OnPolicyTrainer(Trainer)
class class Trainer(metaclass=ABCMeta)
```

这体现了类之间的复用关系，DQN以及其他算法对应的Trainer都需要属性，直接设置在OnPolicyTrainer中。当需要增加新的算法对应的Trainer时，只需要继承自OnPolicyTrainer即可。



* DQN算法核心逻辑

```python
model = getattr(self, 'model')
model.eval()
# TODO: 此段需要详细理解，算法核心处理逻辑
with torch.no_grad():
    q, h = model(_obs, state=None)
    q = q.masked_fill(~_obs_legal, float('-inf'))    
    q_max = q.max(dim=1).values.detach()#.cpu()

target_q = ret + self._gamma * q_max * not_done

self.optim.zero_grad()
# 调用self.model.train()之后，再调用self.model()
# TODO: 此处需要捋清楚，为啥需要再次调用一遍self.model()函数
# 可以研究一下DQN算法的逻辑过程！
frames = self(obs, model_selector="model", model_mode="train")
loss = torch.square(target_q - frames.logits.gather(1, action).view(-1)).sum() 
loss.backward()
self.optim.step()
```

由于之前调用了model.eval()函数，使得模型进入了评估模式，无法更新参数。第9行利用Q值计算完target_q值之后，后续需要更新模型，因此通过self()调用__call__成员函数，其中调用model.train()使模型进入训练模式。



### PPO算法学习理解

与DQN不同，该算法是一种基于策略优化的算法。策略和对应的价值分别对应一个神经网络。PPO通过限制新旧策略之间的差异来稳定训练过程。



[PPO参考内容](https://www.cnblogs.com/antelx/p/17578681.html)



### 强化学习基础

强化学习基础内容为马尔科夫链，参考链接：[链接](https://blog.csdn.net/november_chopin/article/details/106589197)

强化学习的两种模式：

1. 获取每个状态-动作对的期望奖励Q值，智能体通过选择Q值大的动作执行，从而使得累计奖励最大
2. 直接优化策略函数，使得累计奖励值最大



### Hok_env 论文阅读

Hok_env为腾讯开悟开发的王者荣耀AI的模拟开发接口环境，可以方便使用其提供的王者荣耀接口进行强化学习的开发。

现存的RL模型一般针对于特定领域的任务，很难进行泛化。

Hok针对上述问题，提出了一些针对泛化性的挑战：

1. 针对对手的泛化

对于同一个智能体，面对不同的对手智能体，需要具有一定的泛化能力

1. 针对英雄的泛化

不同的英雄具有不同的技能，因此，强化学习需要能够针对不同英雄的泛化能力



做出的贡献：提供了王者荣耀的模拟环境，抽象了复杂的操作，并提供了对应的API。

- 几个基本概念

1. Task：对于一个特定的target hero和一个特定的对手英雄，该局游戏就叫一个task，20个英雄，两两组队，总共可以生成40个任务。
2. Observation Space：the status of a match in one task, include the information of hero, creep, turret and so on.
3. Action Space：a triplet form, indecating which action button to take, who to target, how to act.



后续做了一些实验，介绍了强化学习模型的泛化能力。

[github address](https://github.com/tencent-ailab/hok_env)

[paper address](https://arxiv.org/abs/2209.08483)

### 模型部署

#### 模型简化

原模型首先将图像信息进行卷积处理，英雄数值信息、野怪、小兵、塔、物件等信息进行多层线性层和池化层处理。再将所有信息拼接后送多层卷积层和LSTM层进行处理后输出对应英雄动作。

* 使用LoRA进行模型调试

将Lora与SVD奇异值分解结合，将一个完整的MLP层拆成2个相邻的MLP层，由如下两部分组成：

1. 第一个MLP层不包含Bias，维度为 M*n
2. 第二个MLP层包含Bias，维度为 n*N

用这两个MLP层级联实现Lora效果。我们通过分析Lora前后的对应W矩阵的L2-Loss，可以自适应地对MLP内的W矩阵进行自动化裁剪，这极大程度上为我们早期的模型瓶颈分析提供了便捷：借助Lora技术，我们可以定量分析模型中每一模块的重要程度，从而高效地定位模型瓶颈。由于Lora的低秩性，我们没有将Lora技术生成的级联MLP直接部署至最终模型中，仅将Lora作为过程分析的工具。对于逻辑较为清楚的小型模型，将上述Lora当作工程部署的过程工具比较适合实际的开发流程。

* 调试中简化模型

首先对模型结构进行剪枝处理。在初步调试分析后，我们发现原始模型包含大量不必要的冗余神经元，因此我们选择结构化剪枝作为降低模型复杂度的首要手段。这一过程主要是对模型中的全局线性层、注意力层和时序层进行重新设计，或是直接结构化削减这些模块的规模。

接着，对模型的推理逻辑进行了优化。通过实验我们观察到，训练简化版的模型（不具备时序处理能力）拟合本赛题提供的对局数据集，也能在实际对战中展现出高水平的对战性能，单帧观测输入已经能较好地反映场上的关键信息。因此，在最终提交的参赛模型中，我们直接去除了模型中的时序层，仅使用当前场上的向量化状态作为策略网络的输入。此外，尽管注意力机制在提升模型性能方面起着重要作用，我们还是对注意力层进行了适度缩减，包括降低多头注意力的头数（同时保持单头的维度不变）、减少注意力块的堆叠层数等。同时，我们将显示在图中的四个轴（MONSTER、SOLDIER、ORGAN、WHOLE）上的Pooling层替换为功能等效的线性映射层，以增强模型的训练稳定性。

#### 只采用soft loss进行训练

本次试题按照传统模型蒸馏方法定义了三种训练损失函数：Soft Loss、Hard Loss以及Distillation Loss（一种综合Soft和Hard Loss的混合Loss）。经过分析研究，猜想本模型训练数据或涵盖多个基于**强化学习**的多教师模型下的动作概率分布以及动作标签。本训练任务与传统的蒸馏学习任务略有不同，就动作标签来看，无论是通过采样还是通过取概率分布的最大值，或是使用近似贪婪策略实现，这些硬标签（Hard Label）的信息已经完整地包含在软标签（Soft Label）的动作概率分布中，并不具有Ground Truth的本身意义。也就是说，这些硬标签本质上就来自于这些软标签。基于此，我们决定在此次训练中舍弃Hard Loss和蒸馏Loss，转而仅使用Soft Loss来进行从分布到分布的学习强调了使用输出分布与训练集中的Soft Label之间的交叉熵作为主要的损失函数。团队成员认为，这种做法有助于提高模型的训练稳定性。

#### 端侧量化优化

Linux端采用fp32进行训练，手机端采用fp16进行量化部署。从而减少模型大小。

#### 模型大小

模型从200多M，简化到了小于10M的大小

#### 简化后模型层次

```python
NetworkModel(
  (conv_layers): Sequential(
    (img_feat_conv1): Conv2d(6, 18, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (img_feat_relu1): ReLU()
    (img_feat_maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (img_feat_conv2): Conv2d(18, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (hero_main_mlp): MLP(
    (fc_layers): Sequential(
      (hero_main_mlp_fc1): Linear(in_features=44, out_features=64, bias=True)
      (hero_main_mlp_non_linear1): ReLU()
      (hero_main_mlp_fc2): Linear(in_features=64, out_features=64, bias=True)
      (hero_main_mlp_non_linear2): ReLU()
      (hero_main_mlp_fc3): Linear(in_features=64, out_features=64, bias=True)
      (hero_main_mlp_non_linear3): ReLU()
      (hero_main_mlp_fc4): Linear(in_features=64, out_features=64, bias=True)
      (hero_main_mlp_non_linear4): ReLU()
      (hero_main_mlp_fc5): Linear(in_features=64, out_features=32, bias=True)
    )
  )
  (hero_mlp): MLP(
    (fc_layers): Sequential(
      (hero_mlp_fc1): Linear(in_features=251, out_features=512, bias=True)
      (hero_mlp_non_linear1): ReLU()
      (hero_mlp_fc2): Linear(in_features=512, out_features=512, bias=True)
      (hero_mlp_non_linear2): ReLU()
      (hero_mlp_fc3): Linear(in_features=512, out_features=128, bias=True)
      (hero_mlp_non_linear3): ReLU()
      (hero_mlp_fc4): Linear(in_features=128, out_features=128, bias=True)
      (hero_mlp_non_linear4): ReLU()
    )
  )
  (hero_frd_fc): Sequential(
    (hero_frd_fc): Linear(in_features=128, out_features=64, bias=True)
  )
  (hero_emy_fc): Sequential(
    (hero_emy_fc): Linear(in_features=128, out_features=64, bias=True)
  )
  (soldier_mlp): MLP(
    (fc_layers): Sequential(
      (soldier_mlp_fc1): Linear(in_features=25, out_features=64, bias=True)
      (soldier_mlp_non_linear1): ReLU()
      (soldier_mlp_fc2): Linear(in_features=64, out_features=64, bias=True)
      (soldier_mlp_non_linear2): ReLU()
      (soldier_mlp_fc3): Linear(in_features=64, out_features=64, bias=True)
      (soldier_mlp_non_linear3): ReLU()
    )
  )
  (soldier_frd_fc): Sequential(
    (soldier_frd_fc): Linear(in_features=64, out_features=32, bias=True)
  )
  (soldier_emy_fc): Sequential(
    (soldier_emy_fc): Linear(in_features=64, out_features=32, bias=True)
  )
  (organ_mlp): MLP(
    (fc_layers): Sequential(
      (organ_mlp_fc1): Linear(in_features=29, out_features=64, bias=True)
      (organ_mlp_non_linear1): ReLU()
      (organ_mlp_fc2): Linear(in_features=64, out_features=64, bias=True)
      (organ_mlp_non_linear2): ReLU()
      (organ_mlp_fc3): Linear(in_features=64, out_features=64, bias=True)
      (organ_mlp_non_linear3): ReLU()
    )
  )
  (organ_frd_fc): Sequential(
    (organ_frd_fc): Linear(in_features=64, out_features=32, bias=True)
  )
  (organ_emy_fc): Sequential(
    (organ_emy_fc): Linear(in_features=64, out_features=32, bias=True)
  )
  (monster_mlp): MLP(
    (fc_layers): Sequential(
      (monster_mlp_fc1): Linear(in_features=28, out_features=64, bias=True)
      (monster_mlp_non_linear1): ReLU()
      (monster_mlp_fc2): Linear(in_features=64, out_features=64, bias=True)
      (monster_mlp_non_linear2): ReLU()
      (monster_mlp_fc3): Linear(in_features=64, out_features=64, bias=True)
      (monster_mlp_non_linear3): ReLU()
      (monster_mlp_fc4): Linear(in_features=64, out_features=32, bias=True)
    )
  )
  (concat_mlp): MLP(
    (fc_layers): Sequential(
      (concat_mlp_fc1): Linear(in_features=1156, out_features=256, bias=True)
      (concat_mlp_non_linear1): ReLU()
    )
  )
  (label_mlp): ModuleDict(
    (hero_label0_mlp): MLP(
      (fc_layers): Sequential(
        (hero_label0_mlp_fc1): Linear(in_features=256, out_features=64, bias=True)
        (hero_label0_mlp_non_linear1): ReLU()
        (hero_label0_mlp_fc2): Linear(in_features=64, out_features=13, bias=True)
      )
    )
    (hero_label1_mlp): MLP(
      (fc_layers): Sequential(
        (hero_label1_mlp_fc1): Linear(in_features=256, out_features=64, bias=True)
        (hero_label1_mlp_non_linear1): ReLU()
        (hero_label1_mlp_fc2): Linear(in_features=64, out_features=25, bias=True)
      )
    )
    (hero_label2_mlp): MLP(
      (fc_layers): Sequential(
        (hero_label2_mlp_fc1): Linear(in_features=256, out_features=64, bias=True)
        (hero_label2_mlp_non_linear1): ReLU()
        (hero_label2_mlp_fc2): Linear(in_features=64, out_features=42, bias=True)
      )
    )
    (hero_label3_mlp): MLP(
      (fc_layers): Sequential(
        (hero_label3_mlp_fc1): Linear(in_features=256, out_features=64, bias=True)
        (hero_label3_mlp_non_linear1): ReLU()
        (hero_label3_mlp_fc2): Linear(in_features=64, out_features=42, bias=True)
      )
    )
    (hero_label4_mlp): MLP(
      (fc_layers): Sequential(
        (hero_label4_mlp_fc1): Linear(in_features=256, out_features=64, bias=True)
        (hero_label4_mlp_non_linear1): ReLU()
        (hero_label4_mlp_fc2): Linear(in_features=64, out_features=39, bias=True)
      )
    )
  )
  (value_mlp): MLP(
    (fc_layers): Sequential(
      (hero_value_mlp_fc1): Linear(in_features=256, out_features=64, bias=True)
      (hero_value_mlp_non_linear1): ReLU()
      (hero_value_mlp_fc2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
```



### 参考资料

> 1. [PPO参考内容](https://www.cnblogs.com/antelx/p/17578681.html)
> 2. [亚军方案分享](https://mp.weixin.qq.com/s/QEqclSpjzWnD5OAJZlY5aQ)


---
title: 一二阶段材料汇总
date: 2025-06-29 12:50:19
categories:
  - data storage
tags:
  - 实习项目
---

# 一阶段

在 ocf 中实现二次准入策略。

## 动机

trace 中存在大量只会访问一次的请求，用二次准入过滤掉这些请求，防止其污染缓存空间。

## 技术点

将每个请求划分成若干个 4K 块，然后对每个 4K 块进行哈希处理，判断该 4K 块之前是否出现过。如果某个请求中，之前出现过的 4K 块的数量超过阈值，则该请求准入；否则，该请求不予准入。

核心代码
```c++
bool ocf_history_check_second_chance(uint64_t addr, uint64_t size) {
    // 计算页面对齐的地址
    uint64_t start_addr = PAGE_ALIGN_DOWN(addr);
    uint64_t end_addr = PAGE_ALIGN_DOWN(addr + size - 1);
    uint64_t total_pages = PAGES_IN_REQ(start_addr, end_addr);
    uint64_t hit_pages = 0;

    // 检查历史记录中的命中率
    for (uint64_t curr_addr = start_addr; curr_addr <= end_addr; curr_addr += PAGE_SIZE) {
        if (ocf_history_hash_find(curr_addr)) {
            hit_pages++;
        }
    }

    // 如果命中率低于阈值，将页面添加到历史记录并返回 false
    if ((float)hit_pages / total_pages < HISTORY_HIT_RATIO_THRESHOLD) {
        // 将所有页面添加到历史记录
        for (uint64_t curr_addr = start_addr; curr_addr <= end_addr; curr_addr += PAGE_SIZE) {
            ocf_history_hash_add_addr(curr_addr);
        }
        return false;  // 标记为 1（一次性访问）
    }

    return true;  // 标记为 0（应被缓存）
}
```

哈希表内部采用 LRU 驱逐策略。

### 主要特点：

- 采用 4K 粒度
- 使用哈希表存储访问历史
- 当历史记录超过容量时，实施 LRU 驱逐策略
- 根据负载因子动态调整大小

## 结果

部分 trace 效果明显（一次访问较多的 trace），能有效提升整体带宽

# 二阶段

OTAE 决策树准入算法

## 动机

一阶段提出的二次准入算法基于启发式规则，难以面对动态变化的工作负载。因此提出决策树准入

## 技术点

OTAE（优化树状准入引擎）是一种智能缓存准入策略，利用决策树预测是否应将缓存块准入到缓存中。通过学习历史访问模式并做出智能准入决策，旨在提高缓存效率。

### 特征提取

离线提取特征，每个请求都提取出下列留了个特征

1. Request address - The starting address of the IO request
2. Request size - The size of the IO request in bytes
3. Access timestamp - The time when the request was made
4. Reuse time - Time since last access (in microseconds)
5. Average requests per minute - Current request rate
6. Access frequency - Number of times this address has been accessed

### 标签生成

对于每个请求，按照一阶段的二次准入生成对应的是否准入的标签，用于模型学习。

### 模型训练

利用生成的特征和标签，使用 python 的 sk-learn 库进行监督学习，生成决策树模型参数文件

训练核心代码
```python
def Train(self, X, y):
	param ={'class_weight': [{0:2,1:1}, {0:2.5,1:1}, {0:1.5, 1:1}],
			'max_leaf_nodes': [30, 50, 70, 100],
			'max_depth': [4, 6, 8]
			}

	# most common choice
	#  param = {'class_weight': [{0:2.5, 1:1}], 'max_leaf_nodes': [70]}
	#  param = {'class_weight': [{0:2.5, 1:1}], 'max_leaf_nodes': [30]}

	fscore = make_scorer(fbeta_score, beta = self.Beta(self.cap), pos_label = 1)

	clf = GridSearchCV(DecisionTreeClassifier(criterion='entropy'),
						param_grid=param,
						scoring=fscore,
						#  scoring='roc_auc',
						cv=5,
						refit=True)
	clf.fit(X, y)

	return clf
```

模型文件实例如下

```bash
         2         1
        31
         4         6         3         1         6         3         1         0         1        -2        -2         1         0        -2        -2        -2        -2         1         5        -2        -2        -2        -2         5         3        -2        -2        -2        -2        -2        -2
  300001.5  4.348270  30751540   22528.0  0.000660    3700.0   67584.0  37160488   71680.0      -2.0      -2.0   14336.0  42770173      -2.0      -2.0      -2.0      -2.0   67584.0  1.683634      -2.0      -2.0      -2.0      -2.0  0.000820  13961449      -2.0      -2.0      -2.0      -2.0      -2.0      -2.0
         1         3         5        11        23        17         7         9        15        -1        -1        21        13        -1        -1        -1        -1        19        27        -1        -1        -1        -1        29        25        -1        -1        -1        -1        -1        -1
         2         4         6        12        24        18         8        10        16        -1        -1        22        14        -1        -1        -1        -1        20        28        -1        -1        -1        -1        30        26        -1        -1        -1        -1        -1        -1
  0.083170  0.016856  0.451453  0.002647  0.999282  0.997662  0.339567  0.484994  0.015733  0.659764  0.312901  0.731543  0.002032  0.000823  0.051495  0.000749  0.422598  0.949224  0.999841       1.0  0.776397  0.809917  0.392857  0.999931  0.984375       1.0       0.0  0.971428  0.999872  0.998661       1.0
  0.916829  0.983143  0.548546  0.997352  0.000717  0.002337  0.660432  0.515005  0.984266  0.340235  0.687098  0.268456  0.997967  0.999176  0.948504  0.999250  0.577401  0.050775  0.000158       0.0  0.223602  0.190082  0.607142  6.803183  0.015625       0.0       1.0  0.028571  0.000127  0.001338       0.0

```

从上往下，各行依次为

类别权重

节点数量

特征索引（-1表示叶子节点）：表示每个节点使用哪个特征

每个节点的阈值

左子节点索引

右子节点索引

负类的概率

正类的概率



### 模型应用

加载生成的决策树参数文件，形成 c++ 中对应的决策树类，对每一个请求进行准入判断。

```c++
class DecisionTree {
public:
    int ResponseNode(std::vector<double> &features) {
        int p = 0;
        int predictor = predictor_[p];
        while(predictor >= 0) {
            if (features[predictor] < threshold_[p]) {
                p = left_[p];
            } else {
                p = right_[p];
            }
            predictor = predictor_[p];
        }
        return p;
    }
};
```

## 结果

通过定期对模型进行训练更新，模型可以针对不同的工作负载进行动态调整。
---
title: RL paper survey
date: 2025-07-04 10:03:02
categories:
- [paper]
- [data storage]
tags:
- algorithm
- RL
- 实习项目
---



# 1. ArtMem

ArtMem：Adaptive Migration in Reinforcement Learning-Enabled Tiered Memory

## 背景：

内存系统只靠 DRAM 不足，因此引入 PM 和基于 CXL 的额外内存，形成快慢速的分层内存系统。

但现有分层内存系统中的页面迁移策略不够 adaptive，没有充分考虑到工作负载的动态性，因此引入强化学习策略。

### 内存访问监控

内存访问监控方面一般有三种方式：页表扫描、页错误捕捉、硬件事件采样。

页表扫描通过检查页面表访问位获取访问信息，但扫描开销高，尤其在页面数量庞大时；

页错误捕捉仅监控未命中的页面，信息不完整；

硬件事件采样利用CPU的性能监控单元（PMU）和精确事件采样（PEBS），以低开销捕获全面的访问数据。但是存在硬件依赖。

ArtMem选择第三种方式，通过每个CPU核心的采样线程收集页面访问地址和频率，存储在环形缓冲区，结合指数桶（以2为底）和冷却操作（每200万样本减半计数），高效跟踪访问分布。

### 页迁移指标

页面迁移需要决策是迁移掉访问频率低的，还是迁移掉最近未访问过的？

ArtMem综合考虑访问频率和最近性，以更准确地识别热页面。

最近性通过Linux的活跃页面列表（active list）和非活跃页面列表（inactive list）获取，基于LRU（最近最少使用）机制。

访问频率通过指数移动平均（EMA）计算，平滑短期波动，反映页面的长期访问趋势。

EMA 值反映了频率和最近性的结合。EMA 公式一般来说如下所示。
$$
EMA_t = α·Access_t + (1-α)·EMA_{t-1}
$$
其中，`Access_t`是当前时间步的访问频率，`α`是平滑因子，`EMA_t`是前一时间步的EMA值。

### DRAM 快速访问比例

DRAM 访问比例可以实时反映内存利用率。利用该信息，可以做出一些更加针对性的页放置决策。

因此将此指标设计为强化学习的状态参数。

### 页迁移范围

页迁移范围也是一个比较重要的指标，因此也将其和热页面阈值一起，作为 RL 调节的环境对象属性。

## ArtMem 设计

ArtMem 是一个基于强化学习（RL）的分层内存管理框架，旨在通过动态页面迁移优化内存密集型应用的性能。本文将深入探讨 ArtMem 的强化学习设计，分析其状态、动作、奖励机制以及 Q-learning 实现，揭示其如何通过学习适应复杂内存访问模式。

### 1. 强化学习框架概述

ArtMem 采用 **Q-learning** 作为核心强化学习算法，这是一种无模型（model-free）的方法，通过与环境的交互学习最优策略。RL 框架的五个关键组件（代理、环境、状态、动作、奖励）在 ArtMem 中被精心设计，以适应分层内存系统的动态需求。以下是 ArtMem RL 框架的核心组成：

| RL 组件                 | ArtMem 设计                                                  |
| ----------------------- | ------------------------------------------------------------ |
| **代理（Agent）**       | 运行在 Linux 内核中的 RL 控制器，负责决策页面迁移策略。      |
| **环境（Environment）** | 分层内存系统（DRAM 和 PM），通过 PMU 和 PEBS 提供内存访问信息。 |
| **状态（State）**       | 离散化的 DRAM 访问比率（DRAM access ratio）。                |
| **动作（Action）**      | 调节热页面阈值和页面迁移数量。                               |
| **奖励（Reward）**      | 基于 DRAM 访问比率变化和迁移成本的组合奖励函数。             |

ArtMem 的 RL 设计目标是最大化快速内存（DRAM）的访问比率，同时最小化不必要的页面迁移开销。通过动态调整迁移策略，ArtMem 能够适应不同工作负载的内存访问模式，显著提升性能（论文中提到平均性能提升 114%）。

### 2. 状态设计

#### 2.1 定义

ArtMem 使用 **DRAM 访问比率**（DRAM access ratio）作为状态的唯一指标，表示内存访问中 DRAM 的占比。公式如下：
$$
[r = \frac{\text{DRAM}_{\text{access}} \times k}{\text{DRAM}_{\text{access}} + \text{PM}_{\text{access}}}]
$$

- `DRAM_access`：快速内存（DRAM）的访问次数。
- `PM_access`：慢速内存（PM）的访问次数。
- `k`：离散化参数，将连续的访问比率映射到 11 个离散状态（0 到 10）。

此外，ArtMem 为特殊情况（例如 $$(\text{DRAM}_{\text{access}} = \text{PM}_{\text{access}} = 0)$$，即访问全部命中 CPU 缓存或无事件发生）设置了一个额外状态（(k = 11)），共 12 个状态。

#### 2.2 设计考量

- **系统级状态**：ArtMem 选择系统级别的 DRAM 访问比率，而不是逐页状态，以降低计算复杂度和 Q 表开销。论文指出，逐页状态会导致 Q 表过大，减慢学习速度。
- **离散化**：将连续的访问比率离散化为 12 个状态，平衡了状态空间的表达能力与计算效率。图 15d 显示，(k = 10) 是经验上的最优值，过多或过少状态都会影响性能。
- **实时性**：通过 PMU 和 PEBS 采样，状态更新能够实时反映内存访问模式的变化，确保 RL 代理对环境变化的敏感性。

### 3. 动作设计

ArtMem 定义了两组动作，分别控制迁移策略的两个关键参数：

1. **调节热页面阈值**：
   - 动作集合：{+8, +4, 0, -4, -8}，表示对热页面阈值的调整步长。
   - 热页面阈值决定了哪些页面被认为是“热”页面，符合条件的页面才会被迁移到 DRAM。
   - 最小阈值设为 0.5（经验值），避免探索阶段设置过低阈值导致大量无效迁移（见第 5 节）。
2. **调节页面迁移数量**：
   - 动作集合：{0MB, 16MB, 32MB, 64MB, 128MB, 256MB, 512MB, 1024MB, 2048MB}。
   - 迁移单位为 2MB 大页面（huge page），以减少地址转换开销。
   - 0MB 表示不进行迁移，用于避免不必要的迁移开销。

#### 3.1 动作空间优化

- **两张 Q 表**：ArtMem 使用两张独立的 Q 表分别控制热页面阈值和迁移数量，允许两者协同优化。这种分离设计降低了单一 Q 表的维度，提高了学习效率。
- **动作粒度**：迁移数量从 16MB 到 2048MB 呈倍数增长，覆盖了从小规模到大规模迁移的场景，适合不同工作负载的动态需求。
- **探索与利用**：通过 ε-贪心策略（$$(\varepsilon = 0.5)$$），ArtMem 在探索新动作和利用已有知识之间取得平衡。图 15c 显示，($$\varepsilon = 0.5$$) 是经验上的最优值。

### 4. 奖励设计

奖励函数是 ArtMem RL 框架的核心，指导代理学习最优迁移策略。论文中定义的奖励函数如下：

$$[\text{Reward} = \tau_i - \beta + \lambda (\tau_i - \tau_{i-1})]$$

- **$$(\tau_i)$$**：当前时间步的 DRAM 访问比率（实际值）。
- **$$(\beta)$$**：预期的 DRAM 访问比率（经验参数，范围 1-10，图 15e 显示其对性能的影响）。
- **$$(\lambda)$$**：二值参数，0 表示未发生迁移，1 表示发生迁移。
- **$$(\tau_i - \tau_{i-1})$$**：DRAM 访问比率的变化，反映迁移效果。

#### 4.1 奖励函数解析

- **目标**：奖励函数鼓励将热页面迁移到 DRAM 以提高 ($$\tau_i$$)，同时通过 ($$\lambda$$) 惩罚不必要的迁移（迁移成本）。
- **$$(\beta)$$** 的作用：作为基准，($$\beta$$) 衡量 ($$\tau_i$$) 是否达到预期。过高的 ($$\beta$$) 可能导致代理过于激进，增加无效迁移；过低的 ($$\beta$$) 则可能限制性能优化。
- **$$(\lambda)$$ 的动态性**：当 ($$\lambda = 0$$)（无迁移），奖励仅基于当前 ($$\tau_i - \beta$$)，避免对未发生迁移的场景施加额外惩罚。

#### 4.2 替代奖励函数

论文在 6.3.4 节探讨了基于内存访问延迟（latency）的奖励函数，通过监控每周期的待处理内存请求数近似延迟。实验结果表明：

- 延迟奖励可将平均延迟降低 3.47 倍，但增加了数据收集开销，导致性能略有下降（图 12）。
- DRAM 访问比率奖励更简单且效果更好，适合实时性要求高的场景。

### 5. Q-learning 实现

ArtMem 的 Q-learning 算法基于 Markov 决策过程（MDP），通过迭代更新 Q 表优化迁移策略。其核心流程如下（见第 7 页的算法 1）：

初始化 Q 表为 0
初始化状态 $ \tau_{i-1} = k $ (DRAM 访问比率 100%)

while 程序未结束:
    根据 $ \tau_{i-1} $ 和 ε-贪心策略选择动作 $ a $
    迁移线程根据动作 $ a $ 执行页面迁移
    采样线程观测新状态 $ \tau_i $
    计算奖励: $ \text{Reward} = \tau_i - \beta + \lambda (\tau_i - \tau_{i-1}) $
    更新 Q 值: $ Q(\tau_{i-1}, a) = Q(\tau_{i-1}, a) + \alpha (\text{Reward} + \gamma \max_{a'} Q(\tau_i, a') - Q(\tau_{i-1}, a)) $
    更新状态: $ \tau_{i-1} \leftarrow \tau_i $

#### 5.1 关键超参数

- **学习率 ($$\alpha$$)**：控制新经验对 Q 值的更新幅度。图 15a 显示，($$\alpha = 0.1$$) 是最优值，平衡了学习速度和稳定性。
- **折扣因子 ($$\gamma$$)**：衡量长期奖励的权重。图 15b 表明，($$\gamma = 0.9$$) 能有效平衡短期和长期收益。
- **探索率 ($$\varepsilon$$)**：控制探索与利用的比例，($$\varepsilon = 0.5$$) 提供最佳性能（图 15c）。
- **迁移间隔**：每 10 秒执行一次迁移（图 15f 建议 5-15 秒为最佳范围），避免过频繁的迁移导致性能下降。

#### 5.2 Q 表实现

- 两张 Q 表：
  - 热页面阈值 Q 表：12 个状态 × 5 个动作（±8, ±4, 0）。
  - 迁移数量 Q 表：12 个状态 × 9 个动作（0MB 到 2048MB）。
- **内存开销**：Q 表存储在内核中，内存占用极低（第 6.4 节提到 Q 表计算开销仅为 CPU 的 0.07%）。
- **初始化**：初始 Q 值为 0，首状态设为 (k = 10)（DRAM 访问比率 100%），反映程序启动时数据加载到 DRAM 的情况。

### 6. RL 与内存系统的集成

ArtMem 通过以下机制将 RL 框架无缝集成到分层内存系统中：

- **采样线程**：
  - 使用 PMU 和 PEBS 每 200 次事件采样一次内存访问数据，记录到环形缓冲区。
  - 采样线程更新页面访问频率和 DRAM 访问比率，供 RL 代理计算状态。
  - 采样开销低，仅占 CPU 的 0.02%（第 6.4 节）。
- **迁移线程**：
  - 异步执行页面迁移，基于 RL 代理的动作，从 PM 活跃列表头部迁移页面到 DRAM。
  - 如果 DRAM 空间不足，从 DRAM 非活跃列表尾部降级冷页面。
  - 迁移单位为 2MB 大页面，降低地址转换开销。
- **交互通道**：
  - 通过 Linux 内核的伪文件系统（memory control group）实现环境与代理的交互。
  - 例如，`memory_hit_ratio_show` 文件提供采样数据，`memory_threshold_show` 和 `memory_hit_on_show` 文件传递动作指令。

### 7. RL 的性能与鲁棒性

#### 7.1 性能表现

- **性能提升**：ArtMem 在多种工作负载下实现 35%-172% 的性能提升（第 1 页），平均提升 114%（第 2 页）。
- 工作负载适应性：
  - **SSSP 和 CC**：RL 有效识别热区域，性能提升 16%-31%（图 9）。
  - **DLRM**：通过学习顺序访问模式，提升 10%-19%。
  - **BTree**：性能接近最佳的 Multi-clock，提升 4%-36%。
  - **Liblinear**：初始访问比率下降后快速恢复，平均提升 76%（第 9 页）。
- **迁移效率**：图 11 显示，ArtMem 的迁移量比 MEMTIS 少 0.09 倍，减少了不必要迁移。

#### 7.2 鲁棒性与泛化能力

- **跨工作负载泛化**：图 14 显示，ArtMem 的 Q 表在不同工作负载上仅 7/25 种情况下性能下降超过 10%，表明较强的泛化能力。
- **快速收敛**：当使用次优 Q 表时，ArtMem 平均需 1-6 次迭代达到 96% 的最优性能（第 11 页）。
- **超参数敏感性**：图 15 表明，ArtMem 对超参数（如 ($$\alpha$$)、($$\gamma$$)、($$\varepsilon)$$）的设置较为鲁棒，经验值已接近最优。

#### 7.3 算法对比

- 论文比较了 Q-learning 和 SARSA（第 6.3.5 节，图 13），结果显示两者性能相近，表明 ArtMem 的框架对 RL 算法选择不敏感，增强了其适用性。

### 8. RL 设计的创新点

1. **系统级状态设计**：通过 DRAM 访问比率简化状态空间，降低计算复杂性。
2. **双 Q 表策略**：分离热页面阈值和迁移数量的控制，增强动作灵活性。
3. **动态奖励函数**：结合访问比率和迁移成本，平衡性能与开销。
4. **低开销集成**：通过异步采样和迁移线程，RL 计算对关键路径的影响最小（采样和 Q 表更新开销分别仅占 0.02% 和 0.07% CPU）。
5. **适应性强**：能够处理动态和混合工作负载（图 16c），尤其在复杂访问模式下表现出色。

### 9. 未来改进方向

- **更复杂的奖励函数**：结合延迟、带宽等多维度指标，进一步优化性能（第 6.3.4 节已尝试延迟奖励）。
- **自适应采样周期**：动态调整采样频率以适应不同工作负载的访问密度。
- **多代理 RL**：在多节点 NUMA 或 CXL 系统中引入多代理协作，优化全局迁移策略。
- **深度 RL 扩展**：对于超大规模内存系统，可尝试深度 Q 网络（DQN）以处理更高维的状态空间。

### 10. 结论

ArtMem 的强化学习设计通过 Q-learning 实现了动态、适应性强的页面迁移策略。其简化的状态空间、灵活的动作设计和高效的奖励函数使其在分层内存系统中表现出色。无论是图算法、推荐系统还是内存数据库，ArtMem 都能通过学习优化内存访问性能，同时保持低开销和强鲁棒性。这一设计为 RL 在系统优化领域的应用提供了宝贵经验，值得进一步探索和扩展。



### 11. 架构图分析

Artem 架构图如下：

<img src="\img\ArtMem architecute.png" alt="ArtMem architecute" style="zoom: 67%;" />

PMU（Performance Monitoring Unit）监控内存访问情况。



ArtMem 总览图：

<img src="\img\overview of ArtMem.png" alt="overview of ArtMem" style="zoom:67%;" />

图中左上角是内存访问次数分布图，记录了每个页面的访问次数。由页面阈值和迁移范围两个参数共同决定了最后的实际迁移范围。

右侧的 DRAM 层和 PM 层都维护了一个基于 LRU 排序的活跃和不活跃的页面队列。也就是架构图中提到的 Page sorting 操作。

异步的迁移线程收到迁移指令之后，从 DRAM 的不活跃列表中从尾部向前进行降级，当 DRAM 空间足够之后，再从 PM 活跃队列头开始依次往上升级。



Q：在实际迁移过程中，是否需要判断当前正在迁移的页面的访问次数是否小于热页面阈值呢？

A：论文中没提，大概率不需要，因此这里的热页面阈值可能只是辅助调整迁移数量参数。



## 扩展思考

### 创新点

1. 强化学习驱动，具有动态自适应性。
2. 系统级粒度，不对某个具体页面做决策。通过监控系统中 DRAM 的访问比例来调节热页面阈值和迁移数量的参数。
3. 存储空间压缩率高。指数桶存储页面访问分布，利用 Linux 大页的 struct page 中未使用字段存储页面访问信息。
4. 用户空间 RL 通过伪文件系统与操作系统交互。
5. 异步处理。采样线程和页面迁移线程都在后天异步执行，不影响主 IO 性能。





## 关联文章

1. CHROME: Concurrency-Aware Holistic Cache Management Framework with Online Reinforcement Learning

利用强化学习对缓存行为做决策。



## 待更新内容

1. 结合 IO 路径再捋一下 RL 具体如何发挥作用


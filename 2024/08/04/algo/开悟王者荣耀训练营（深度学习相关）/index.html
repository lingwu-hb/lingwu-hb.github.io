<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>开悟王者荣耀训练营学习内容 | Hexo</title><meta name="author" content="Bo Han"><meta name="copyright" content="Bo Han"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="开悟训练营学习内容 KaiwuDRL框架是腾讯开悟基于python研发的用于异步的off-policy强化学习训练的分布式框架，可以稳定高效地完成王者荣耀场景下的强化学习相关任务。  图1：KaiwuDRL框架的核心功能模块 AI Server从Battle Server中直接获取到的信息为：  当前帧数 英雄信息 地图中物件的信息  物件分为两种，宝箱和增益Buff。具体细节如下图所示：  图2">
<meta property="og:type" content="article">
<meta property="og:title" content="开悟王者荣耀训练营学习内容">
<meta property="og:url" content="https://lingwu-hb.github.io/2024/08/04/algo/%E5%BC%80%E6%82%9F%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80%E8%AE%AD%E7%BB%83%E8%90%A5%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%EF%BC%89/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="开悟训练营学习内容 KaiwuDRL框架是腾讯开悟基于python研发的用于异步的off-policy强化学习训练的分布式框架，可以稳定高效地完成王者荣耀场景下的强化学习相关任务。  图1：KaiwuDRL框架的核心功能模块 AI Server从Battle Server中直接获取到的信息为：  当前帧数 英雄信息 地图中物件的信息  物件分为两种，宝箱和增益Buff。具体细节如下图所示：  图2">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://lingwu-hb.github.io/img/yaojie.jpg">
<meta property="article:published_time" content="2024-08-04T07:25:47.000Z">
<meta property="article:modified_time" content="2025-03-02T14:01:20.891Z">
<meta property="article:author" content="Bo Han">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="DQN">
<meta property="article:tag" content="PPO">
<meta property="article:tag" content="模型量化">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lingwu-hb.github.io/img/yaojie.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "开悟王者荣耀训练营学习内容",
  "url": "https://lingwu-hb.github.io/2024/08/04/algo/%E5%BC%80%E6%82%9F%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80%E8%AE%AD%E7%BB%83%E8%90%A5%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%EF%BC%89/",
  "image": "https://lingwu-hb.github.io/img/yaojie.jpg",
  "datePublished": "2024-08-04T07:25:47.000Z",
  "dateModified": "2025-03-02T14:01:20.891Z",
  "author": [
    {
      "@type": "Person",
      "name": "Bo Han",
      "url": "https://lingwu-hb.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://lingwu-hb.github.io/2024/08/04/algo/%E5%BC%80%E6%82%9F%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80%E8%AE%AD%E7%BB%83%E8%90%A5%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%EF%BC%89/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;700&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '开悟王者荣耀训练营学习内容',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Hexo</span></a><a class="nav-page-title" href="/"><span class="site-name">开悟王者荣耀训练营学习内容</span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">开悟王者荣耀训练营学习内容</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-08-04T07:25:47.000Z" title="Created 2024-08-04 15:25:47">2024-08-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-03-02T14:01:20.891Z" title="Updated 2025-03-02 22:01:20">2025-03-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h3 id="开悟训练营学习内容">开悟训练营学习内容</h3>
<p>KaiwuDRL框架是腾讯开悟基于python研发的用于异步的off-policy强化学习训练的分布式框架，可以稳定高效地完成王者荣耀场景下的强化学习相关任务。</p>
<p><img src="https://doc.aiarena.tencent.com/kaiwu-arena/gorgewalk_v2/latest/assets/images/kaiwuDRL_framework-5dfa599b4f303143984e9c1201729b6f.png" alt=""></p>
<p>图1：KaiwuDRL框架的核心功能模块</p>
<p>AI Server从Battle Server中直接获取到的信息为：</p>
<ol>
<li>当前帧数</li>
<li>英雄信息</li>
<li>地图中物件的信息</li>
</ol>
<p>物件分为两种，宝箱和增益Buff。具体细节如下图所示：</p>
<p><img src="D:%5CpostGraRel%5CCompetition%5CtencentKaiWuAlgorithm%5Cimg%5CSingleReq%E7%BB%93%E6%9E%84UML%E5%9B%BE.png" alt="SingleReq结构UML图"></p>
<p>图2：SingleReq数据结构UML图</p>
<p>AI Server为Battle Server和agent之间的桥梁，为两端进行数据传输。</p>
<p>其中，我们需要重点考察的为特征处理部分内容和奖励设置部分，相关修改策略见腾讯开悟文档。</p>
<ul>
<li>特征处理</li>
</ul>
<p>特征处理包括常规处理（app/gorge_walk/state_action_reward/gorge_walk_obs_reward.py）和神经网络的处理（app/gorge_walk/algorithm/torch_network.py）中的CNN网络。</p>
<ul>
<li>奖励设置</li>
</ul>
<p>奖励部分见（app/gorge_walk/state_action_reward/gorge_walk_obs_reward.py）</p>
<h4 id="代码执行流程分析">代码执行流程分析</h4>
<p>以Learner为例，分析app/gorge_walk中代码具体如何与框架中代码进行交互</p>
<ol>
<li>启动相关进程</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># app/gorge_walk/train_test.py</span></span><br><span class="line"><span class="comment"># 启动aisrv, learner, actor, battlesrv</span></span><br><span class="line">procs : <span class="type">List</span>[Process] = []</span><br><span class="line">procs.append(Process(target=learner.main, name=<span class="string">&quot;learner&quot;</span>))</span><br><span class="line">procs.append(Process(target=actor.main, name=<span class="string">&quot;actor&quot;</span>))</span><br><span class="line">procs.append(Process(target=aisrv.main, name=<span class="string">&quot;aisrv&quot;</span>))</span><br><span class="line">procs.append(Process(target=battlesrv.main, name=<span class="string">&#x27;battlesrv&#x27;</span>))</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>加载相关神经网络和模型</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># framework/server/python/learner/on_policy_trainer.py</span></span><br><span class="line"><span class="comment"># network (for DQN, the network is BaseCNNNetwork in app/gorge_walk/algorithm/torch_network.py)</span></span><br><span class="line">network = self.policy_conf.learner_network(</span><br><span class="line">    self.policy_conf.state.state_space(),</span><br><span class="line">    self.policy_conf.action.action_space()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model</span></span><br><span class="line">name = <span class="string">&quot;%s_%s&quot;</span> % (CONFIG.app, CONFIG.algo)</span><br><span class="line"><span class="comment"># for DQN, the model is DQN model in app/gorge_walk/algorithm/torch_dqn_model.py</span></span><br><span class="line">self.model = AlgoConf[CONFIG.algo].learner_model(</span><br><span class="line">    network, name, CONFIG.svr_name)</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>Learner开始执行</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loop() -&gt; run_once() -&gt; self.train() -&gt; self.model_wrapper.train() -&gt; DQNmodel.learn()</span><br></pre></td></tr></table></figure>
<h4 id="Q-A">Q&amp;A</h4>
<p>Q：目前需要做的工作就是：修改奖励设置 + 修改算法模型？具体而言，应该朝着什么方向去修改呢？</p>
<p>Q：如何设置奖励和惩罚的数值呢？完全按照经验嘛？</p>
<p>Q：这个项目的重点是奖励的设置？还是算法的修改？哪一部分对算法效果的影响更大？</p>
<p>Q：定义了一个BaseNetwork和一个BaseCNNNetwork，后者用了作为特征提取，那该网络的输出是什么呢？又是在什么地方使用了该模型的输出呢？前者都作为其他模型（DQN、PPO以及diy）的一部分</p>
<p>A：网络和模型不同，模型用于测试和训练，一般由多个网络组成。网络继承自nn.Module虚拟类，只要实现了__init__()和forward()即可。</p>
<h3 id="DQN算法学习理解">DQN算法学习理解</h3>
<p>DQN（Deep Q-Network）算法基于Q-learning算法，使用了神经网络替代了Q-learning算法中的Q Tables。</p>
<ul>
<li>Q-learning算法简介</li>
</ul>
<p>Q Tables表示为Q(s, a)，即在状态s下，执行动作a，<strong>所期望获得的未来奖励</strong>。</p>
<p>Q-learning算法的核心就在于<strong>不断得更新Q Tables矩阵，直至Q Tables值得到收敛。</strong></p>
<p>在每个时间步，智能体都需要选择一个行动。这可以通过一种叫做ε-greedy策略的方法来完成，该方法会在探索（随机选择行动）和利用（选择当前Q-value最高的行动）之间进行权衡。</p>
<p>当智能体选择一个动作执行，并得到结果之后，智能体就按照贝尔曼等式对Q Tables进行更新。</p>
<ul>
<li>DQN算法</li>
</ul>
<p>当环境中的状态和动作非常多的时候，Q Tables无法将所有的状态-动作对所表示的奖励值都存储起来。使用深度网络模型代替Q Tables，深度网络的输入为状态s，输出为对应各个可能动作的Q值。</p>
<p>通过这种方式，我们就可以在连续的状态空间和大规模的动作空间中工作。（<strong>相当于把一个Q Tables，变成了一个Q函数，函数所对应的输入当然是连续的！</strong>）</p>
<p>[DQN参考博客](<a target="_blank" rel="noopener" href="https://www.cnblogs.com/xoslh/p/17609512.html#:~:text=DQN%EF%BC%8C%E5%85%A8%E7%A7%B0Deep">https://www.cnblogs.com/xoslh/p/17609512.html#:~:text=DQN，全称Deep</a> Q-Network，是一种强化学习算法，由DeepMind于2015年首次提出。,它结合了深度学习和Q学习两种技术，可以解决具有大量状态和动作的复杂问题。 在传统的Q-learning中，我们用一个表（Q-table）来存储每个状态-动作对的Q值。)</p>
<p>在开悟模型中，Actor与AI Server进行交互，拿到当前环境的状态，然后利用模型算法进行预测，产生next_state；Actor本身并不会进行训练更新模型参数，而是周期性得负责Learner模型的参数。</p>
<p>Learner从AI Server处拿到批量样本，训练模型并更新参数（一般使用梯度下降算法），然后周期性同步到Actor中。</p>
<p>对于DQN算法的实现，在框架代码中，需要<strong>理解Trainer的作用</strong>，同时，对于DQNTrainer的实现，框架代码使用了<strong>某种设计模式</strong>，对Trainer进行了层层嵌套。</p>
<p>从上往下，分为三层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GorgeWalkDQNTrainer</span>(<span class="title class_ inherited__">OnPolicyTrainer</span>)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">OnPolicyTrainer</span>(<span class="title class_ inherited__">Trainer</span>)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">class</span> Trainer(metaclass=ABCMeta)</span><br></pre></td></tr></table></figure>
<p>这体现了类之间的复用关系，DQN以及其他算法对应的Trainer都需要属性，直接设置在OnPolicyTrainer中。当需要增加新的算法对应的Trainer时，只需要继承自OnPolicyTrainer即可。</p>
<ul>
<li>DQN算法核心逻辑</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">model = <span class="built_in">getattr</span>(self, <span class="string">&#x27;model&#x27;</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> 此段需要详细理解，算法核心处理逻辑</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    q, h = model(_obs, state=<span class="literal">None</span>)</span><br><span class="line">    q = q.masked_fill(~_obs_legal, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>))    </span><br><span class="line">    q_max = q.<span class="built_in">max</span>(dim=<span class="number">1</span>).values.detach()<span class="comment">#.cpu()</span></span><br><span class="line"></span><br><span class="line">target_q = ret + self._gamma * q_max * not_done</span><br><span class="line"></span><br><span class="line">self.optim.zero_grad()</span><br><span class="line"><span class="comment"># 调用self.model.train()之后，再调用self.model()</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> 此处需要捋清楚，为啥需要再次调用一遍self.model()函数</span></span><br><span class="line"><span class="comment"># 可以研究一下DQN算法的逻辑过程！</span></span><br><span class="line">frames = self(obs, model_selector=<span class="string">&quot;model&quot;</span>, model_mode=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">loss = torch.square(target_q - frames.logits.gather(<span class="number">1</span>, action).view(-<span class="number">1</span>)).<span class="built_in">sum</span>() </span><br><span class="line">loss.backward()</span><br><span class="line">self.optim.step()</span><br></pre></td></tr></table></figure>
<p>由于之前调用了model.eval()函数，使得模型进入了评估模式，无法更新参数。第9行利用Q值计算完target_q值之后，后续需要更新模型，因此通过self()调用__call__成员函数，其中调用model.train()使模型进入训练模式。</p>
<h3 id="PPO算法学习理解">PPO算法学习理解</h3>
<p>与DQN不同，该算法是一种基于策略优化的算法。策略和对应的价值分别对应一个神经网络。PPO通过限制新旧策略之间的差异来稳定训练过程。</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/antelx/p/17578681.html">PPO参考内容</a></p>
<h3 id="强化学习基础">强化学习基础</h3>
<p>强化学习基础内容为马尔科夫链，参考链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/november_chopin/article/details/106589197">链接</a></p>
<p>强化学习的两种模式：</p>
<ol>
<li>获取每个状态-动作对的期望奖励Q值，智能体通过选择Q值大的动作执行，从而使得累计奖励最大</li>
<li>直接优化策略函数，使得累计奖励值最大</li>
</ol>
<h3 id="Hok-env-论文阅读">Hok_env 论文阅读</h3>
<p>Hok_env为腾讯开悟开发的王者荣耀AI的模拟开发接口环境，可以方便使用其提供的王者荣耀接口进行强化学习的开发。</p>
<p>现存的RL模型一般针对于特定领域的任务，很难进行泛化。</p>
<p>Hok针对上述问题，提出了一些针对泛化性的挑战：</p>
<ol>
<li>针对对手的泛化</li>
</ol>
<p>对于同一个智能体，面对不同的对手智能体，需要具有一定的泛化能力</p>
<ol>
<li>针对英雄的泛化</li>
</ol>
<p>不同的英雄具有不同的技能，因此，强化学习需要能够针对不同英雄的泛化能力</p>
<p>做出的贡献：提供了王者荣耀的模拟环境，抽象了复杂的操作，并提供了对应的API。</p>
<ul>
<li>几个基本概念</li>
</ul>
<ol>
<li>Task：对于一个特定的target hero和一个特定的对手英雄，该局游戏就叫一个task，20个英雄，两两组队，总共可以生成40个任务。</li>
<li>Observation Space：the status of a match in one task, include the information of hero, creep, turret and so on.</li>
<li>Action Space：a triplet form, indecating which action button to take, who to target, how to act.</li>
</ol>
<p>后续做了一些实验，介绍了强化学习模型的泛化能力。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/tencent-ailab/hok_env">github address</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2209.08483">paper address</a></p>
<h3 id="模型部署">模型部署</h3>
<h4 id="模型简化">模型简化</h4>
<p>原模型首先将图像信息进行卷积处理，英雄数值信息、野怪、小兵、塔、物件等信息进行多层线性层和池化层处理。再将所有信息拼接后送多层卷积层和LSTM层进行处理后输出对应英雄动作。</p>
<ul>
<li>使用LoRA进行模型调试</li>
</ul>
<p>将Lora与SVD奇异值分解结合，将一个完整的MLP层拆成2个相邻的MLP层，由如下两部分组成：</p>
<ol>
<li>第一个MLP层不包含Bias，维度为 M*n</li>
<li>第二个MLP层包含Bias，维度为 n*N</li>
</ol>
<p>用这两个MLP层级联实现Lora效果。我们通过分析Lora前后的对应W矩阵的L2-Loss，可以自适应地对MLP内的W矩阵进行自动化裁剪，这极大程度上为我们早期的模型瓶颈分析提供了便捷：借助Lora技术，我们可以定量分析模型中每一模块的重要程度，从而高效地定位模型瓶颈。由于Lora的低秩性，我们没有将Lora技术生成的级联MLP直接部署至最终模型中，仅将Lora作为过程分析的工具。对于逻辑较为清楚的小型模型，将上述Lora当作工程部署的过程工具比较适合实际的开发流程。</p>
<ul>
<li>调试中简化模型</li>
</ul>
<p>首先对模型结构进行剪枝处理。在初步调试分析后，我们发现原始模型包含大量不必要的冗余神经元，因此我们选择结构化剪枝作为降低模型复杂度的首要手段。这一过程主要是对模型中的全局线性层、注意力层和时序层进行重新设计，或是直接结构化削减这些模块的规模。</p>
<p>接着，对模型的推理逻辑进行了优化。通过实验我们观察到，训练简化版的模型（不具备时序处理能力）拟合本赛题提供的对局数据集，也能在实际对战中展现出高水平的对战性能，单帧观测输入已经能较好地反映场上的关键信息。因此，在最终提交的参赛模型中，我们直接去除了模型中的时序层，仅使用当前场上的向量化状态作为策略网络的输入。此外，尽管注意力机制在提升模型性能方面起着重要作用，我们还是对注意力层进行了适度缩减，包括降低多头注意力的头数（同时保持单头的维度不变）、减少注意力块的堆叠层数等。同时，我们将显示在图中的四个轴（MONSTER、SOLDIER、ORGAN、WHOLE）上的Pooling层替换为功能等效的线性映射层，以增强模型的训练稳定性。</p>
<h4 id="只采用soft-loss进行训练">只采用soft loss进行训练</h4>
<p>本次试题按照传统模型蒸馏方法定义了三种训练损失函数：Soft Loss、Hard Loss以及Distillation Loss（一种综合Soft和Hard Loss的混合Loss）。经过分析研究，猜想本模型训练数据或涵盖多个基于<strong>强化学习</strong>的多教师模型下的动作概率分布以及动作标签。本训练任务与传统的蒸馏学习任务略有不同，就动作标签来看，无论是通过采样还是通过取概率分布的最大值，或是使用近似贪婪策略实现，这些硬标签（Hard Label）的信息已经完整地包含在软标签（Soft Label）的动作概率分布中，并不具有Ground Truth的本身意义。也就是说，这些硬标签本质上就来自于这些软标签。基于此，我们决定在此次训练中舍弃Hard Loss和蒸馏Loss，转而仅使用Soft Loss来进行从分布到分布的学习强调了使用输出分布与训练集中的Soft Label之间的交叉熵作为主要的损失函数。团队成员认为，这种做法有助于提高模型的训练稳定性。</p>
<h4 id="端侧量化优化">端侧量化优化</h4>
<p>Linux端采用fp32进行训练，手机端采用fp16进行量化部署。从而减少模型大小。</p>
<h4 id="模型大小">模型大小</h4>
<p>模型从200多M，简化到了小于10M的大小</p>
<h4 id="简化后模型层次">简化后模型层次</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line">NetworkModel(</span><br><span class="line">  (conv_layers): Sequential(</span><br><span class="line">    (img_feat_conv1): Conv2d(<span class="number">6</span>, <span class="number">18</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    (img_feat_relu1): ReLU()</span><br><span class="line">    (img_feat_maxpool1): MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (img_feat_conv2): Conv2d(<span class="number">18</span>, <span class="number">12</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  )</span><br><span class="line">  (hero_main_mlp): MLP(</span><br><span class="line">    (fc_layers): Sequential(</span><br><span class="line">      (hero_main_mlp_fc1): Linear(in_features=<span class="number">44</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (hero_main_mlp_non_linear1): ReLU()</span><br><span class="line">      (hero_main_mlp_fc2): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (hero_main_mlp_non_linear2): ReLU()</span><br><span class="line">      (hero_main_mlp_fc3): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (hero_main_mlp_non_linear3): ReLU()</span><br><span class="line">      (hero_main_mlp_fc4): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (hero_main_mlp_non_linear4): ReLU()</span><br><span class="line">      (hero_main_mlp_fc5): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">32</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (hero_mlp): MLP(</span><br><span class="line">    (fc_layers): Sequential(</span><br><span class="line">      (hero_mlp_fc1): Linear(in_features=<span class="number">251</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (hero_mlp_non_linear1): ReLU()</span><br><span class="line">      (hero_mlp_fc2): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (hero_mlp_non_linear2): ReLU()</span><br><span class="line">      (hero_mlp_fc3): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">128</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (hero_mlp_non_linear3): ReLU()</span><br><span class="line">      (hero_mlp_fc4): Linear(in_features=<span class="number">128</span>, out_features=<span class="number">128</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (hero_mlp_non_linear4): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (hero_frd_fc): Sequential(</span><br><span class="line">    (hero_frd_fc): Linear(in_features=<span class="number">128</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (hero_emy_fc): Sequential(</span><br><span class="line">    (hero_emy_fc): Linear(in_features=<span class="number">128</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (soldier_mlp): MLP(</span><br><span class="line">    (fc_layers): Sequential(</span><br><span class="line">      (soldier_mlp_fc1): Linear(in_features=<span class="number">25</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (soldier_mlp_non_linear1): ReLU()</span><br><span class="line">      (soldier_mlp_fc2): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (soldier_mlp_non_linear2): ReLU()</span><br><span class="line">      (soldier_mlp_fc3): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (soldier_mlp_non_linear3): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (soldier_frd_fc): Sequential(</span><br><span class="line">    (soldier_frd_fc): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">32</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (soldier_emy_fc): Sequential(</span><br><span class="line">    (soldier_emy_fc): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">32</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (organ_mlp): MLP(</span><br><span class="line">    (fc_layers): Sequential(</span><br><span class="line">      (organ_mlp_fc1): Linear(in_features=<span class="number">29</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (organ_mlp_non_linear1): ReLU()</span><br><span class="line">      (organ_mlp_fc2): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (organ_mlp_non_linear2): ReLU()</span><br><span class="line">      (organ_mlp_fc3): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (organ_mlp_non_linear3): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (organ_frd_fc): Sequential(</span><br><span class="line">    (organ_frd_fc): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">32</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (organ_emy_fc): Sequential(</span><br><span class="line">    (organ_emy_fc): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">32</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (monster_mlp): MLP(</span><br><span class="line">    (fc_layers): Sequential(</span><br><span class="line">      (monster_mlp_fc1): Linear(in_features=<span class="number">28</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (monster_mlp_non_linear1): ReLU()</span><br><span class="line">      (monster_mlp_fc2): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (monster_mlp_non_linear2): ReLU()</span><br><span class="line">      (monster_mlp_fc3): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (monster_mlp_non_linear3): ReLU()</span><br><span class="line">      (monster_mlp_fc4): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">32</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (concat_mlp): MLP(</span><br><span class="line">    (fc_layers): Sequential(</span><br><span class="line">      (concat_mlp_fc1): Linear(in_features=<span class="number">1156</span>, out_features=<span class="number">256</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (concat_mlp_non_linear1): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (label_mlp): ModuleDict(</span><br><span class="line">    (hero_label0_mlp): MLP(</span><br><span class="line">      (fc_layers): Sequential(</span><br><span class="line">        (hero_label0_mlp_fc1): Linear(in_features=<span class="number">256</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (hero_label0_mlp_non_linear1): ReLU()</span><br><span class="line">        (hero_label0_mlp_fc2): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">13</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (hero_label1_mlp): MLP(</span><br><span class="line">      (fc_layers): Sequential(</span><br><span class="line">        (hero_label1_mlp_fc1): Linear(in_features=<span class="number">256</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (hero_label1_mlp_non_linear1): ReLU()</span><br><span class="line">        (hero_label1_mlp_fc2): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">25</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (hero_label2_mlp): MLP(</span><br><span class="line">      (fc_layers): Sequential(</span><br><span class="line">        (hero_label2_mlp_fc1): Linear(in_features=<span class="number">256</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (hero_label2_mlp_non_linear1): ReLU()</span><br><span class="line">        (hero_label2_mlp_fc2): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">42</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (hero_label3_mlp): MLP(</span><br><span class="line">      (fc_layers): Sequential(</span><br><span class="line">        (hero_label3_mlp_fc1): Linear(in_features=<span class="number">256</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (hero_label3_mlp_non_linear1): ReLU()</span><br><span class="line">        (hero_label3_mlp_fc2): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">42</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (hero_label4_mlp): MLP(</span><br><span class="line">      (fc_layers): Sequential(</span><br><span class="line">        (hero_label4_mlp_fc1): Linear(in_features=<span class="number">256</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (hero_label4_mlp_non_linear1): ReLU()</span><br><span class="line">        (hero_label4_mlp_fc2): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">39</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (value_mlp): MLP(</span><br><span class="line">    (fc_layers): Sequential(</span><br><span class="line">      (hero_value_mlp_fc1): Linear(in_features=<span class="number">256</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (hero_value_mlp_non_linear1): ReLU()</span><br><span class="line">      (hero_value_mlp_fc2): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="参考资料">参考资料</h3>
<blockquote>
<ol>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/antelx/p/17578681.html">PPO参考内容</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/QEqclSpjzWnD5OAJZlY5aQ">亚军方案分享</a></li>
</ol>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://lingwu-hb.github.io">Bo Han</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://lingwu-hb.github.io/2024/08/04/algo/%E5%BC%80%E6%82%9F%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80%E8%AE%AD%E7%BB%83%E8%90%A5%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%EF%BC%89/">https://lingwu-hb.github.io/2024/08/04/algo/%E5%BC%80%E6%82%9F%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80%E8%AE%AD%E7%BB%83%E8%90%A5%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/DQN/">DQN</a><a class="post-meta__tags" href="/tags/PPO/">PPO</a><a class="post-meta__tags" href="/tags/%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96/">模型量化</a></div><div class="post-share"><div class="social-share" data-image="/img/yaojie.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/08/04/AICache/cache-placement-survey/" title="cache placement survey"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">cache placement survey</div></div><div class="info-2"><div class="info-item-1">数据放置类文章笔记 数据放置问题一般产生于分布式存储系统中，多节点之间的数据均衡问题 对于文件系统而言，一般的负载均衡策略都基于文件级（还有块级、目录级别、客户端请求级别、混合级别等） 文件级别的负载均衡大概分为两类，基于哈希的负载均衡和基于启发式策略的负载均衡，后期一般需要结合机器学习 2010 A data placement strategy in scientific cloud workflows 利用数据依赖矩阵表示数据之间的依赖关系，尽量将相互依赖的数据放置在同一个数据节点上，从而使得应用在请求数据时，能够快速获取当前需要的数据以及与其相关的数据。 2015 基于分布式哈希/数据放置 Adaptive Data Placement For Staging-Based Coupled Scientific Workflows 根据特定应用的动态数据访问模式调整数据放置，并应用访问模式驱动和位置感知机制来降低数据访问成本，支持多个工作流组件之间的高效数据共享。 2018 分层存储/数据迁移 Stacker: An Autonomic Data Movement...</div></div></div></a><a class="pagination-related" href="/2024/08/05/paper/Paper-reading-guidance/" title="Paper reading guidance"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">Paper reading guidance</div></div><div class="info-2"><div class="info-item-1">文献检索 文献来源 CCF A &amp; B 会议或期刊论文，主要是A类会议 CCF推荐会议期刊推荐目录（2022） 检索工具  dblp (推荐) 谷歌学术 会议官方网站 大佬主页 connected papers (做survey与写文献综述的时候很有用) semantic scholar arXiv 其他：知乎等  其他工具  文献管理软件：Zotero (推荐) 文献阅读：ChatPDF等 会议投稿时间：会伴 谷歌插件：CCF Rank 论文写作：Overleaf，Grammarly，ChatGPT  一些经验 文献阅读 精读与泛读  精读，形成文本并保存   论文背景 解决了什么问题 解决问题的思路 使用了什么方法，提出了什么架构 实验结果分析 有什么收获或启发 (关注一下是否开源数据，源码等方便复现)   泛读，做survey的时候需要，广泛快速阅读大量文献，主要关注1，2，3，4点  服务器 可以申请一台属于自己的服务器，所有实验数据，源码，原型系统等都可以放在上面 </div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/yaojie.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Bo Han</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">28</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">31</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%80%E6%82%9F%E8%AE%AD%E7%BB%83%E8%90%A5%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9"><span class="toc-number">1.</span> <span class="toc-text">开悟训练营学习内容</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90"><span class="toc-number">1.1.</span> <span class="toc-text">代码执行流程分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Q-A"><span class="toc-number">1.2.</span> <span class="toc-text">Q&amp;A</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DQN%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%90%86%E8%A7%A3"><span class="toc-number">2.</span> <span class="toc-text">DQN算法学习理解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PPO%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%90%86%E8%A7%A3"><span class="toc-number">3.</span> <span class="toc-text">PPO算法学习理解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="toc-number">4.</span> <span class="toc-text">强化学习基础</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hok-env-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB"><span class="toc-number">5.</span> <span class="toc-text">Hok_env 论文阅读</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2"><span class="toc-number">6.</span> <span class="toc-text">模型部署</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%AE%80%E5%8C%96"><span class="toc-number">6.1.</span> <span class="toc-text">模型简化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%AA%E9%87%87%E7%94%A8soft-loss%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83"><span class="toc-number">6.2.</span> <span class="toc-text">只采用soft loss进行训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AB%AF%E4%BE%A7%E9%87%8F%E5%8C%96%E4%BC%98%E5%8C%96"><span class="toc-number">6.3.</span> <span class="toc-text">端侧量化优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%A4%A7%E5%B0%8F"><span class="toc-number">6.4.</span> <span class="toc-text">模型大小</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%80%E5%8C%96%E5%90%8E%E6%A8%A1%E5%9E%8B%E5%B1%82%E6%AC%A1"><span class="toc-number">6.5.</span> <span class="toc-text">简化后模型层次</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">7.</span> <span class="toc-text">参考资料</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/29/%E5%BC%80%E9%A2%98/%E5%89%8D%E6%9C%9F%E8%B0%83%E7%A0%94/" title="前期调研">前期调研</a><time datetime="2025-06-29T05:00:30.000Z" title="Created 2025-06-29 13:00:30">2025-06-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/29/AICache/tsPrefetcher%20%E8%B0%83%E7%A0%94/" title="tsPrefetcher 调研">tsPrefetcher 调研</a><time datetime="2025-06-29T04:55:08.000Z" title="Created 2025-06-29 12:55:08">2025-06-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/29/AICache/%E4%B8%80%E4%BA%8C%E9%98%B6%E6%AE%B5%E6%9D%90%E6%96%99%E6%B1%87%E6%80%BB/" title="一二阶段材料汇总">一二阶段材料汇总</a><time datetime="2025-06-29T04:50:19.000Z" title="Created 2025-06-29 12:50:19">2025-06-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/11/%E9%9A%8F%E7%AC%94/useful-tools/" title="useful tools">useful tools</a><time datetime="2025-06-11T03:18:27.000Z" title="Created 2025-06-11 11:18:27">2025-06-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/06/algo/attention-mechanism/" title="attention mechanism">attention mechanism</a><time datetime="2025-06-06T02:05:14.000Z" title="Created 2025-06-06 10:05:14">2025-06-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Bo Han</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.2.0</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>